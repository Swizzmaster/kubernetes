From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Chao Chen <chaochn@amazon.com>
Date: Mon, 4 Apr 2022 12:11:21 -0700
Subject: [PATCH] --EKS-PRIVATE-- Optional maximum etcd page size on every list
 requests

---
 .../apiserver/pkg/server/options/etcd.go      |   3 +
 .../pkg/storage/etcd3/metrics/metrics.go      |  15 ++
 .../apiserver/pkg/storage/etcd3/store.go      | 254 ++++++++++--------
 .../apiserver/pkg/storage/etcd3/store_test.go | 120 ++++++++-
 .../pkg/storage/etcd3/watcher_test.go         |   8 +-
 .../pkg/storage/storagebackend/config.go      |   2 +
 .../storage/storagebackend/factory/etcd3.go   |   3 +-
 .../pkg/storage/tests/cacher_test.go          |   4 +-
 8 files changed, 279 insertions(+), 130 deletions(-)

diff --git a/staging/src/k8s.io/apiserver/pkg/server/options/etcd.go b/staging/src/k8s.io/apiserver/pkg/server/options/etcd.go
index 2c1c27e1592..baeae618a44 100644
--- a/staging/src/k8s.io/apiserver/pkg/server/options/etcd.go
+++ b/staging/src/k8s.io/apiserver/pkg/server/options/etcd.go
@@ -189,6 +189,9 @@ func (s *EtcdOptions) AddFlags(fs *pflag.FlagSet) {
 	fs.DurationVar(&s.StorageConfig.HealthcheckTimeout, "etcd-healthcheck-timeout", s.StorageConfig.HealthcheckTimeout,
 		"The timeout to use when checking etcd health.")
 
+	fs.IntVar(&s.StorageConfig.MaximumPageSize, "maximum-page-size-for-etcd-lists", s.StorageConfig.MaximumPageSize,
+		"Maximum etcd page size for etcd lists. If zero, the limit is unbounded")
+
 	fs.Int64Var(&s.StorageConfig.LeaseManagerConfig.ReuseDurationSeconds, "lease-reuse-duration-seconds", s.StorageConfig.LeaseManagerConfig.ReuseDurationSeconds,
 		"The time in seconds that each lease is reused. A lower value could avoid large number of objects reusing the same lease. Notice that a too small value may cause performance problems at storage layer.")
 }
diff --git a/staging/src/k8s.io/apiserver/pkg/storage/etcd3/metrics/metrics.go b/staging/src/k8s.io/apiserver/pkg/storage/etcd3/metrics/metrics.go
index c97a4afc823..cde4259de51 100644
--- a/staging/src/k8s.io/apiserver/pkg/storage/etcd3/metrics/metrics.go
+++ b/staging/src/k8s.io/apiserver/pkg/storage/etcd3/metrics/metrics.go
@@ -85,6 +85,15 @@ var (
 		},
 		[]string{},
 	)
+	listStorageLatency = compbasemetrics.NewHistogramVec(
+		&compbasemetrics.HistogramOpts{
+			Name:           "apiserver_storage_list_duration_seconds",
+			Help:           "Duration of objects returned for a LIST request from storage",
+			Buckets:        []float64{0.005, 0.025, 0.1, 0.25, 0.5, 1.0, 2.0, 4.0, 15.0, 30.0, 60.0},
+			StabilityLevel: compbasemetrics.ALPHA,
+		},
+		[]string{"type"},
+	)
 )
 
 var registerMetrics sync.Once
@@ -99,6 +108,7 @@ func Register() {
 		legacyregistry.MustRegister(dbTotalSize)
 		legacyregistry.MustRegister(etcdBookmarkCounts)
 		legacyregistry.MustRegister(etcdLeaseObjectCounts)
+		legacyregistry.MustRegister(listStorageLatency)
 	})
 }
 
@@ -113,6 +123,11 @@ func RecordEtcdRequestLatency(verb, resource string, startTime time.Time) {
 	etcdRequestLatency.WithLabelValues(verb, resource).Observe(sinceInSeconds(startTime))
 }
 
+// RecordListStorageLatency sets the "apiserver_storage_list_duration_seconds" metrics.
+func RecordListStorageLatency(typename string, startTime time.Time) {
+	listStorageLatency.WithLabelValues(typename).Observe(sinceInSeconds(startTime))
+}
+
 // RecordEtcdBookmark updates the etcd_bookmark_counts metric.
 func RecordEtcdBookmark(resource string) {
 	etcdBookmarkCounts.WithLabelValues(resource).Inc()
diff --git a/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store.go b/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store.go
index 1bd9bc8cbb6..4ce381f535d 100644
--- a/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store.go
+++ b/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store.go
@@ -23,6 +23,7 @@ import (
 	"encoding/json"
 	"errors"
 	"fmt"
+	"math"
 	"path"
 	"reflect"
 	"strings"
@@ -64,14 +65,20 @@ func (d authenticatedDataString) AuthenticatedData() []byte {
 var _ value.Context = authenticatedDataString("")
 
 type store struct {
-	client        *clientv3.Client
-	codec         runtime.Codec
-	versioner     storage.Versioner
-	transformer   value.Transformer
-	pathPrefix    string
-	watcher       *watcher
-	pagingEnabled bool
-	leaseManager  *leaseManager
+	client       *clientv3.Client
+	codec        runtime.Codec
+	versioner    storage.Versioner
+	transformer  value.Transformer
+	pathPrefix   string
+	watcher      *watcher
+	leaseManager *leaseManager
+
+	pagingConfig PagingConfig
+}
+
+type PagingConfig struct {
+	PagingEnabled   bool
+	MaximumPageSize int
 }
 
 type objState struct {
@@ -83,23 +90,23 @@ type objState struct {
 }
 
 // New returns an etcd3 implementation of storage.Interface.
-func New(c *clientv3.Client, codec runtime.Codec, newFunc func() runtime.Object, prefix string, transformer value.Transformer, pagingEnabled bool, leaseManagerConfig LeaseManagerConfig) storage.Interface {
-	return newStore(c, codec, newFunc, prefix, transformer, pagingEnabled, leaseManagerConfig)
+func New(c *clientv3.Client, codec runtime.Codec, newFunc func() runtime.Object, prefix string, transformer value.Transformer, leaseManagerConfig LeaseManagerConfig, pagingConfig PagingConfig) storage.Interface {
+	return newStore(c, codec, newFunc, prefix, transformer, leaseManagerConfig, pagingConfig)
 }
 
-func newStore(c *clientv3.Client, codec runtime.Codec, newFunc func() runtime.Object, prefix string, transformer value.Transformer, pagingEnabled bool, leaseManagerConfig LeaseManagerConfig) *store {
+func newStore(c *clientv3.Client, codec runtime.Codec, newFunc func() runtime.Object, prefix string, transformer value.Transformer, leaseManagerConfig LeaseManagerConfig, pagingConfig PagingConfig) *store {
 	versioner := APIObjectVersioner{}
 	result := &store{
-		client:        c,
-		codec:         codec,
-		versioner:     versioner,
-		transformer:   transformer,
-		pagingEnabled: pagingEnabled,
+		client:      c,
+		codec:       codec,
+		versioner:   versioner,
+		transformer: transformer,
 		// for compatibility with etcd2 impl.
 		// no-op for default prefix of '/registry'.
 		// keeps compatibility with etcd2 impl for custom prefixes that don't start with '/'
 		pathPrefix:   path.Join("/", prefix),
 		watcher:      newWatcher(c, codec, newFunc, versioner, transformer),
+		pagingConfig: pagingConfig,
 		leaseManager: newDefaultLeaseManager(c, leaseManagerConfig),
 	}
 	return result
@@ -603,6 +610,108 @@ func encodeContinue(key, keyPrefix string, resourceVersion int64) (string, error
 	return base64.RawURLEncoding.EncodeToString(out), nil
 }
 
+func (s *store) paginatedList(ctx context.Context, trace *utiltrace.Trace, key, end string, rev int64, fromRev *uint64,
+	v reflect.Value, typeName string, pred storage.SelectionPredicate, newItemFunc func() runtime.Object) (hasMore bool, startKey string, resourceVersion, remainingCnt int64, err error) {
+	maximumLimit := int64(s.pagingConfig.MaximumPageSize)
+	if maximumLimit <= 0 {
+		maximumLimit = math.MaxInt64
+	}
+	if !s.pagingConfig.PagingEnabled || pred.Limit <= 0 {
+		pred.Limit = math.MaxInt64
+	}
+
+	// loop until we have filled the requested limit from etcd or there are no more results
+	var lastKey []byte
+	var getResp *clientv3.GetResponse
+	var pages float64
+	start := time.Now()
+	defer func() {
+		if err == nil {
+			// quantify the list from storage latency
+			metrics.RecordListStorageLatency(typeName, start)
+		}
+		// only log number of pages if the request is slow >= 500ms
+		trace.Step("fetched all pages from etcd", utiltrace.Field{Key: "page-count", Value: pages})
+	}()
+	for {
+		limit := pred.Limit - int64(v.Len())
+		if maximumLimit < limit {
+			limit = maximumLimit
+		}
+
+		opts := []clientv3.OpOption{clientv3.WithRange(end), clientv3.WithRev(rev), clientv3.WithLimit(limit)}
+		startTime := time.Now()
+		getResp, err = s.client.KV.Get(ctx, key, opts...)
+		if len(end) == 0 {
+			metrics.RecordEtcdRequestLatency("get", typeName, startTime)
+		} else {
+			metrics.RecordEtcdRequestLatency("list", typeName, startTime)
+		}
+		if err != nil {
+			return false, "", 0, 0, err
+		}
+
+		pages++
+		// check ResourceVersionMatchNotOlderThan
+		if fromRev != nil && *fromRev > uint64(getResp.Header.Revision) {
+			return false, "", 0, 0, storage.NewTooLargeResourceVersionError(uint64(rev), uint64(getResp.Header.Revision), 0)
+		}
+		hasMore = getResp.More
+
+		if len(getResp.Kvs) == 0 && getResp.More {
+			return false, "", 0, 0, fmt.Errorf("no results were found, but etcd indicated there were more values remaining")
+		}
+
+		// set up remainingCnt and resourceVersion when the first query returns
+		if remainingCnt == 0 && getResp.Count-pred.Limit > 0 {
+			remainingCnt = getResp.Count - pred.Limit
+		}
+		if rev == 0 {
+			rev = getResp.Header.Revision
+		}
+
+		// avoid small allocations for the result slice, since this can be called in many
+		// different contexts and we don't know how significantly the result will be filtered
+		if pred.Empty() {
+			growSlice(v, len(getResp.Kvs))
+		} else {
+			growSlice(v, 2048, len(getResp.Kvs))
+		}
+
+		// take items from the response until the bucket is full, filtering as we go
+		for i, kv := range getResp.Kvs {
+			if int64(v.Len()) >= pred.Limit {
+				hasMore = true
+				break
+			}
+			lastKey = kv.Key
+
+			data, _, err := s.transformer.TransformFromStorage(kv.Value, authenticatedDataString(kv.Key))
+			if err != nil {
+				return false, "", 0, 0, storage.NewInternalErrorf("unable to transform key %q: %v", kv.Key, err)
+			}
+
+			if err := appendListItem(v, data, uint64(kv.ModRevision), pred, s.codec, s.versioner, newItemFunc); err != nil {
+				return false, "", 0, 0, err
+			}
+
+			// free kv early. Long lists can take O(seconds) to decode.
+			getResp.Kvs[i] = nil
+		}
+
+		// no more results remain
+		if !hasMore {
+			break
+		}
+		// we have filled our bucket
+		if int64(v.Len()) >= pred.Limit {
+			break
+		}
+		key = string(lastKey) + "\x00"
+	}
+	return hasMore, string(lastKey) + "\x00", rev, remainingCnt, err
+}
+
 // List implements storage.Interface.List.
 func (s *store) List(ctx context.Context, key string, opts storage.ListOptions, listObj runtime.Object) error {
 	resourceVersion := opts.ResourceVersion
@@ -635,14 +744,6 @@ func (s *store) List(ctx context.Context, key string, opts storage.ListOptions,
 	}
 	keyPrefix := key
 
-	// set the appropriate clientv3 options to filter the returned data set
-	var paging bool
-	options := make([]clientv3.OpOption, 0, 4)
-	if s.pagingEnabled && pred.Limit > 0 {
-		paging = true
-		options = append(options, clientv3.WithLimit(pred.Limit))
-	}
-
 	newItemFunc := getNewItemFunc(listObj, v)
 
 	var fromRV *uint64
@@ -654,10 +755,10 @@ func (s *store) List(ctx context.Context, key string, opts storage.ListOptions,
 		fromRV = &parsedRV
 	}
 
-	var returnedRV, continueRV, withRev int64
-	var continueKey string
+	var continueRV, withRev int64
+	var continueKey, rangeEnd string
 	switch {
-	case s.pagingEnabled && len(pred.Continue) > 0:
+	case s.pagingConfig.PagingEnabled && len(pred.Continue) > 0:
 		continueKey, continueRV, err = decodeContinue(pred.Continue, keyPrefix)
 		if err != nil {
 			return apierrors.NewBadRequest(fmt.Sprintf("invalid continue token: %v", err))
@@ -667,8 +768,7 @@ func (s *store) List(ctx context.Context, key string, opts storage.ListOptions,
 			return apierrors.NewBadRequest("specifying resource version is not allowed when using continue")
 		}
 
-		rangeEnd := clientv3.GetPrefixRangeEnd(keyPrefix)
-		options = append(options, clientv3.WithRange(rangeEnd))
+		rangeEnd = clientv3.GetPrefixRangeEnd(keyPrefix)
 		key = continueKey
 
 		// If continueRV > 0, the LIST request needs a specific resource version.
@@ -676,29 +776,25 @@ func (s *store) List(ctx context.Context, key string, opts storage.ListOptions,
 		// If continueRV < 0, the request is for the latest resource version.
 		if continueRV > 0 {
 			withRev = continueRV
-			returnedRV = continueRV
 		}
-	case s.pagingEnabled && pred.Limit > 0:
+	case s.pagingConfig.PagingEnabled && pred.Limit > 0:
 		if fromRV != nil {
 			switch match {
 			case metav1.ResourceVersionMatchNotOlderThan:
 				// The not older than constraint is checked after we get a response from etcd,
 				// and returnedRV is then set to the revision we get from the etcd response.
 			case metav1.ResourceVersionMatchExact:
-				returnedRV = int64(*fromRV)
-				withRev = returnedRV
+				withRev = int64(*fromRV)
 			case "": // legacy case
 				if *fromRV > 0 {
-					returnedRV = int64(*fromRV)
-					withRev = returnedRV
+					withRev = int64(*fromRV)
 				}
 			default:
 				return fmt.Errorf("unknown ResourceVersionMatch value: %v", match)
 			}
 		}
 
-		rangeEnd := clientv3.GetPrefixRangeEnd(keyPrefix)
-		options = append(options, clientv3.WithRange(rangeEnd))
+		rangeEnd = clientv3.GetPrefixRangeEnd(keyPrefix)
 	default:
 		if fromRV != nil {
 			switch match {
@@ -706,91 +802,26 @@ func (s *store) List(ctx context.Context, key string, opts storage.ListOptions,
 				// The not older than constraint is checked after we get a response from etcd,
 				// and returnedRV is then set to the revision we get from the etcd response.
 			case metav1.ResourceVersionMatchExact:
-				returnedRV = int64(*fromRV)
-				withRev = returnedRV
+				withRev = int64(*fromRV)
 			case "": // legacy case
 			default:
 				return fmt.Errorf("unknown ResourceVersionMatch value: %v", match)
 			}
 		}
 
-		options = append(options, clientv3.WithPrefix())
-	}
-	if withRev != 0 {
-		options = append(options, clientv3.WithRev(withRev))
+		rangeEnd = clientv3.GetPrefixRangeEnd(key)
 	}
 
-	// loop until we have filled the requested limit from etcd or there are no more results
-	var lastKey []byte
-	var hasMore bool
-	var getResp *clientv3.GetResponse
-	for {
-		startTime := time.Now()
-		getResp, err = s.client.KV.Get(ctx, key, options...)
-		metrics.RecordEtcdRequestLatency("list", getTypeName(listPtr), startTime)
-		if err != nil {
-			return interpretListError(err, len(pred.Continue) > 0, continueKey, keyPrefix)
-		}
-		if err = s.validateMinimumResourceVersion(resourceVersion, uint64(getResp.Header.Revision)); err != nil {
-			return err
-		}
-		hasMore = getResp.More
-
-		if len(getResp.Kvs) == 0 && getResp.More {
-			return fmt.Errorf("no results were found, but etcd indicated there were more values remaining")
-		}
-
-		// avoid small allocations for the result slice, since this can be called in many
-		// different contexts and we don't know how significantly the result will be filtered
-		if pred.Empty() {
-			growSlice(v, len(getResp.Kvs))
-		} else {
-			growSlice(v, 2048, len(getResp.Kvs))
-		}
-
-		// take items from the response until the bucket is full, filtering as we go
-		for _, kv := range getResp.Kvs {
-			if paging && int64(v.Len()) >= pred.Limit {
-				hasMore = true
-				break
-			}
-			lastKey = kv.Key
-
-			data, _, err := s.transformer.TransformFromStorage(kv.Value, authenticatedDataString(kv.Key))
-			if err != nil {
-				return storage.NewInternalErrorf("unable to transform key %q: %v", kv.Key, err)
-			}
-
-			if err := appendListItem(v, data, uint64(kv.ModRevision), pred, s.codec, s.versioner, newItemFunc); err != nil {
-				return err
-			}
-		}
-
-		// indicate to the client which resource version was returned
-		if returnedRV == 0 {
-			returnedRV = getResp.Header.Revision
-		}
-
-		// no more results remain or we didn't request paging
-		if !hasMore || !paging {
-			break
-		}
-		// we're paging but we have filled our bucket
-		if int64(v.Len()) >= pred.Limit {
-			break
-		}
-		key = string(lastKey) + "\x00"
-		if withRev == 0 {
-			withRev = returnedRV
-			options = append(options, clientv3.WithRev(withRev))
-		}
+	hasMore, startKey, rv, remainingCnt, err := s.paginatedList(ctx, trace, key, rangeEnd, withRev, fromRV, v, getTypeName(listObj), opts.Predicate, newItemFunc)
+	if err != nil {
+		return interpretListError(err, len(pred.Continue) > 0, continueKey, keyPrefix)
 	}
 
 	// instruct the client to begin querying from immediately after the last key we returned
 	// we never return a key that the client wouldn't be allowed to see
 	if hasMore {
 		// we want to start immediately after the last key
-		next, err := encodeContinue(string(lastKey)+"\x00", keyPrefix, returnedRV)
+		next, err := encodeContinue(startKey, keyPrefix, rv)
 		if err != nil {
 			return err
 		}
@@ -800,15 +831,14 @@ func (s *store) List(ctx context.Context, key string, opts storage.ListOptions,
 		// Only set remainingItemCount if the predicate is empty.
 		if utilfeature.DefaultFeatureGate.Enabled(features.RemainingItemCount) {
 			if pred.Empty() {
-				c := int64(getResp.Count - pred.Limit)
-				remainingItemCount = &c
+				remainingItemCount = &remainingCnt
 			}
 		}
-		return s.versioner.UpdateList(listObj, uint64(returnedRV), next, remainingItemCount)
+		return s.versioner.UpdateList(listObj, uint64(rv), next, remainingItemCount)
 	}
 
 	// no continuation
-	return s.versioner.UpdateList(listObj, uint64(returnedRV), "", nil)
+	return s.versioner.UpdateList(listObj, uint64(rv), "", nil)
 }
 
 // growSlice takes a slice value and grows its capacity up
diff --git a/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store_test.go b/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store_test.go
index a432abf4a73..964f185695e 100644
--- a/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store_test.go
+++ b/staging/src/k8s.io/apiserver/pkg/storage/etcd3/store_test.go
@@ -57,7 +57,10 @@ import (
 var scheme = runtime.NewScheme()
 var codecs = serializer.NewCodecFactory(scheme)
 
-const defaultTestPrefix = "test!"
+const (
+	defaultTestPrefix       = "test!"
+	defaultListEtcdMaxLimit = 500
+)
 
 func init() {
 	metav1.AddToGroupVersion(scheme, metav1.SchemeGroupVersion)
@@ -1017,7 +1020,8 @@ func TestTransformationFailure(t *testing.T) {
 	codec := apitesting.TestCodec(codecs, examplev1.SchemeGroupVersion)
 	cluster := integration.NewClusterV3(t, &integration.ClusterConfig{Size: 1})
 	defer cluster.Terminate(t)
-	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, false, NewDefaultLeaseManagerConfig())
+	pagingConfig := PagingConfig{PagingEnabled: false, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, NewDefaultLeaseManagerConfig(), pagingConfig)
 	ctx := context.Background()
 
 	preset := []struct {
@@ -1094,8 +1098,10 @@ func TestList(t *testing.T) {
 	codec := apitesting.TestCodec(codecs, examplev1.SchemeGroupVersion)
 	cluster := integration.NewClusterV3(t, &integration.ClusterConfig{Size: 1})
 	defer cluster.Terminate(t)
-	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, true, NewDefaultLeaseManagerConfig())
-	disablePagingStore := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, false, NewDefaultLeaseManagerConfig())
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, NewDefaultLeaseManagerConfig(), pagingConfig)
+	disablePagingConfig := PagingConfig{PagingEnabled: false, MaximumPageSize: defaultListEtcdMaxLimit}
+	disablePagingStore := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, NewDefaultLeaseManagerConfig(), disablePagingConfig)
 	ctx := context.Background()
 
 	// Setup storage with the following structure:
@@ -1585,6 +1591,87 @@ func TestList(t *testing.T) {
 	}
 }
 
+func TestListPaginationWithEnforcedLimit(t *testing.T) {
+	cluster := integration.NewClusterV3(t, &integration.ClusterConfig{Size: 1})
+	defer cluster.Terminate(t)
+	etcdClient := cluster.RandClient()
+	codec := apitesting.TestCodec(codecs, examplev1.SchemeGroupVersion)
+	transformer := &prefixTransformer{prefix: []byte(defaultTestPrefix)}
+	recorder := &clientRecorder{KV: etcdClient.KV}
+	etcdClient.KV = recorder
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(etcdClient, codec, newPod, "", transformer, NewDefaultLeaseManagerConfig(), pagingConfig)
+	ctx := context.Background()
+
+	// write 1000 pods
+	podCount := 1000
+	var pods []*example.Pod
+	for i := 0; i < podCount; i++ {
+		key := fmt.Sprintf("/one-level/pod-%d", i)
+		obj := &example.Pod{ObjectMeta: metav1.ObjectMeta{Name: fmt.Sprintf("pod-%d", i)}}
+		storedObj := &example.Pod{}
+		err := store.Create(ctx, key, obj, storedObj, 0)
+		if err != nil {
+			t.Fatalf("Set failed: %v", err)
+		}
+		pods = append(pods, storedObj)
+	}
+
+	// user list requests with 100 limit + latest resource version
+	options := storage.ListOptions{
+		ResourceVersion: "0",
+		Predicate: storage.SelectionPredicate{
+			Limit: 100,
+			Label: labels.Everything(),
+			Field: fields.OneTermEqualSelector("metadata.name", "pod-999"),
+			GetAttrs: func(obj runtime.Object) (labels.Set, fields.Set, error) {
+				pod := obj.(*example.Pod)
+				return nil, fields.Set{"metadata.name": pod.Name}, nil
+			},
+		},
+	}
+
+	testCases := []struct {
+		// limit is the apiserver enforced list etcd page size limit
+		limit int
+		pages int
+	}{
+		{
+			limit: 1,
+			pages: 1000,
+		},
+		{
+			limit: 20,
+			pages: 50,
+		},
+		{
+			limit: 500, // use user specified 100 limit
+			pages: 10,
+		},
+	}
+	for _, tc := range testCases {
+		store.pagingConfig.MaximumPageSize = tc.limit
+		out := &example.PodList{}
+		if err := store.List(ctx, "/", options, out); err != nil {
+			t.Fatalf("Unable to get initial list: %v", err)
+		}
+		if len(out.Continue) != 0 {
+			t.Errorf("Unexpected continuation token set")
+		}
+		if len(out.Items) != 1 || !reflect.DeepEqual(&out.Items[0], pods[999]) {
+			t.Fatalf("Unexpected first page: %#v", out.Items)
+		}
+		if transformer.reads != uint64(podCount) {
+			t.Errorf("unexpected reads: %d", transformer.reads)
+		}
+		if int(recorder.reads) != tc.pages {
+			t.Errorf("expect reads: %d, but got %d", tc.pages, recorder.reads)
+		}
+		transformer.resetReads()
+		recorder.resetReads()
+	}
+}
+
 func TestListContinuation(t *testing.T) {
 	codec := apitesting.TestCodec(codecs, examplev1.SchemeGroupVersion)
 	cluster := integration.NewClusterV3(t, &integration.ClusterConfig{Size: 1})
@@ -1593,7 +1680,8 @@ func TestListContinuation(t *testing.T) {
 	etcdClient := cluster.RandClient()
 	recorder := &clientRecorder{KV: etcdClient.KV}
 	etcdClient.KV = recorder
-	store := newStore(etcdClient, codec, newPod, "", transformer, true, NewDefaultLeaseManagerConfig())
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(etcdClient, codec, newPod, "", transformer, NewDefaultLeaseManagerConfig(), pagingConfig)
 	ctx := context.Background()
 
 	// Setup storage with the following structure:
@@ -1755,7 +1843,8 @@ func TestListContinuationWithFilter(t *testing.T) {
 	etcdClient := cluster.RandClient()
 	recorder := &clientRecorder{KV: etcdClient.KV}
 	etcdClient.KV = recorder
-	store := newStore(etcdClient, codec, newPod, "", transformer, true, NewDefaultLeaseManagerConfig())
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(etcdClient, codec, newPod, "", transformer, NewDefaultLeaseManagerConfig(), pagingConfig)
 	ctx := context.Background()
 
 	preset := []struct {
@@ -1858,7 +1947,8 @@ func TestListInconsistentContinuation(t *testing.T) {
 	codec := apitesting.TestCodec(codecs, examplev1.SchemeGroupVersion)
 	cluster := integration.NewClusterV3(t, &integration.ClusterConfig{Size: 1})
 	defer cluster.Terminate(t)
-	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, true, NewDefaultLeaseManagerConfig())
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, NewDefaultLeaseManagerConfig(), pagingConfig)
 	ctx := context.Background()
 
 	// Setup storage with the following structure:
@@ -2006,10 +2096,11 @@ func testSetup(t *testing.T) (context.Context, *store, *integration.ClusterV3) {
 	// As 30s is the default timeout for testing in glboal configuration,
 	// we cannot wait longer than that in a single time: change it to 10
 	// for testing purposes. See apimachinery/pkg/util/wait/wait.go
-	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, true, LeaseManagerConfig{
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, LeaseManagerConfig{
 		ReuseDurationSeconds: 1,
 		MaxObjectCount:       defaultLeaseMaxObjectCount,
-	})
+	}, pagingConfig)
 	ctx := context.Background()
 	return ctx, store, cluster
 }
@@ -2050,8 +2141,9 @@ func TestPrefix(t *testing.T) {
 		"/custom//prefix//": "/custom/prefix",
 		"/registry":         "/registry",
 	}
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
 	for configuredPrefix, effectivePrefix := range testcases {
-		store := newStore(cluster.RandClient(), codec, nil, configuredPrefix, transformer, true, NewDefaultLeaseManagerConfig())
+		store := newStore(cluster.RandClient(), codec, nil, configuredPrefix, transformer, NewDefaultLeaseManagerConfig(), pagingConfig)
 		if store.pathPrefix != effectivePrefix {
 			t.Errorf("configured prefix of %s, expected effective prefix of %s, got %s", configuredPrefix, effectivePrefix, store.pathPrefix)
 		}
@@ -2217,7 +2309,8 @@ func TestConsistentList(t *testing.T) {
 	transformer := &fancyTransformer{
 		transformer: &prefixTransformer{prefix: []byte(defaultTestPrefix)},
 	}
-	store := newStore(cluster.RandClient(), codec, newPod, "", transformer, true, NewDefaultLeaseManagerConfig())
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(cluster.RandClient(), codec, newPod, "", transformer, NewDefaultLeaseManagerConfig(), pagingConfig)
 	transformer.store = store
 
 	for i := 0; i < 5; i++ {
@@ -2322,10 +2415,11 @@ func TestCount(t *testing.T) {
 func TestLeaseMaxObjectCount(t *testing.T) {
 	codec := apitesting.TestCodec(codecs, examplev1.SchemeGroupVersion)
 	cluster := integration.NewClusterV3(t, &integration.ClusterConfig{Size: 1})
-	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, true, LeaseManagerConfig{
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, LeaseManagerConfig{
 		ReuseDurationSeconds: defaultLeaseReuseDurationSeconds,
 		MaxObjectCount:       2,
-	})
+	}, pagingConfig)
 	ctx := context.Background()
 	defer cluster.Terminate(t)
 
diff --git a/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher_test.go b/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher_test.go
index a141aaafc38..9ff1fc3c140 100644
--- a/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher_test.go
+++ b/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher_test.go
@@ -225,13 +225,14 @@ func TestWatchError(t *testing.T) {
 	codec := &testCodec{apitesting.TestCodec(codecs, examplev1.SchemeGroupVersion)}
 	cluster := integration.NewClusterV3(t, &integration.ClusterConfig{Size: 1})
 	defer cluster.Terminate(t)
-	invalidStore := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte("test!")}, true, NewDefaultLeaseManagerConfig())
+	pagingConfig := PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	invalidStore := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte("test!")}, NewDefaultLeaseManagerConfig(), pagingConfig)
 	ctx := context.Background()
 	w, err := invalidStore.Watch(ctx, "/abc", storage.ListOptions{ResourceVersion: "0", Predicate: storage.Everything})
 	if err != nil {
 		t.Fatalf("Watch failed: %v", err)
 	}
-	validStore := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte("test!")}, true, NewDefaultLeaseManagerConfig())
+	validStore := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte("test!")}, NewDefaultLeaseManagerConfig(), pagingConfig)
 	validStore.GuaranteedUpdate(ctx, "/abc", &example.Pod{}, true, nil, storage.SimpleUpdate(
 		func(runtime.Object) (runtime.Object, error) {
 			return &example.Pod{ObjectMeta: metav1.ObjectMeta{Name: "foo"}}, nil
@@ -321,7 +322,8 @@ func TestProgressNotify(t *testing.T) {
 	}
 	cluster := integration.NewClusterV3(t, clusterConfig)
 	defer cluster.Terminate(t)
-	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, false, NewDefaultLeaseManagerConfig())
+	pagingConfig := PagingConfig{PagingEnabled: false, MaximumPageSize: defaultListEtcdMaxLimit}
+	store := newStore(cluster.RandClient(), codec, newPod, "", &prefixTransformer{prefix: []byte(defaultTestPrefix)}, NewDefaultLeaseManagerConfig(), pagingConfig)
 	ctx := context.Background()
 
 	key := "/somekey"
diff --git a/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/config.go b/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/config.go
index 1036be45404..76136e6573b 100644
--- a/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/config.go
+++ b/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/config.go
@@ -80,6 +80,8 @@ type Config struct {
 	// HealthcheckTimeout specifies the timeout used when checking health
 	HealthcheckTimeout time.Duration
 
+	MaximumPageSize int
+
 	LeaseManagerConfig etcd3.LeaseManagerConfig
 }
 
diff --git a/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go b/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go
index 83aa620ddce..2c12d10143c 100644
--- a/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go
+++ b/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go
@@ -260,7 +260,8 @@ func newETCD3Storage(c storagebackend.Config, newFunc func() runtime.Object) (st
 	if transformer == nil {
 		transformer = value.IdentityTransformer
 	}
-	return etcd3.New(client, c.Codec, newFunc, c.Prefix, transformer, c.Paging, c.LeaseManagerConfig), destroyFunc, nil
+	pagingConfig := etcd3.PagingConfig{PagingEnabled: c.Paging, MaximumPageSize: c.MaximumPageSize}
+	return etcd3.New(client, c.Codec, newFunc, c.Prefix, transformer, c.LeaseManagerConfig, pagingConfig), destroyFunc, nil
 }
 
 // startDBSizeMonitorPerEndpoint starts a loop to monitor etcd database size and update the
diff --git a/staging/src/k8s.io/apiserver/pkg/storage/tests/cacher_test.go b/staging/src/k8s.io/apiserver/pkg/storage/tests/cacher_test.go
index 67920d90d19..ff5bbb3b4b8 100644
--- a/staging/src/k8s.io/apiserver/pkg/storage/tests/cacher_test.go
+++ b/staging/src/k8s.io/apiserver/pkg/storage/tests/cacher_test.go
@@ -61,6 +61,7 @@ var (
 const (
 	// watchCacheDefaultCapacity syncs watch cache defaultLowerBoundCapacity.
 	watchCacheDefaultCapacity = 100
+	defaultListEtcdMaxLimit   = 500
 )
 
 func init() {
@@ -105,7 +106,8 @@ func newPodList() runtime.Object { return &example.PodList{} }
 
 func newEtcdTestStorage(t *testing.T, prefix string) (*etcd3testing.EtcdTestServer, storage.Interface) {
 	server, _ := etcd3testing.NewUnsecuredEtcd3TestClientServer(t)
-	storage := etcd3.New(server.V3Client, apitesting.TestCodec(codecs, examplev1.SchemeGroupVersion), newPod, prefix, value.IdentityTransformer, true, etcd3.NewDefaultLeaseManagerConfig())
+	pagingConfig := etcd3.PagingConfig{PagingEnabled: true, MaximumPageSize: defaultListEtcdMaxLimit}
+	storage := etcd3.New(server.V3Client, apitesting.TestCodec(codecs, examplev1.SchemeGroupVersion), newPod, prefix, value.IdentityTransformer, etcd3.NewDefaultLeaseManagerConfig(), pagingConfig)
 	return server, storage
 }
 
