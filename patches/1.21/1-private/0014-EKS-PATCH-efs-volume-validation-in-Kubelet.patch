From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Prithvi Ramesh <pritrame@amazon.com>
Date: Sun, 26 Jul 2020 22:25:54 -0700
Subject: [PATCH] --EKS-PATCH-- efs volume validation in Kubelet

--EKS-PRIVATE--

cr https://code.amazon.com/reviews/CR-30564575
---
 pkg/kubelet/kubelet.go                        |   2 +-
 .../lifecycle/fargate_pod_admit_handler.go    |   5 +-
 .../lifecycle/fargate_pod_validator.go        |  97 +++++++++--
 .../lifecycle/fargate_pod_validator_test.go   | 154 ++++++++++++++++++
 .../invalid-daemon-sets.yaml                  |  31 ++++
 .../invalid-default-scheduler.yaml            |  24 +++
 .../invalid-host-network.yaml                 |  25 +++
 .../invalid-pod-securitycontext.yaml          |  27 +++
 .../invalid-ports.yaml                        |  19 +++
 .../invalid-security-context.yaml             |  27 +++
 .../invalid-spec-fields.yaml                  |  24 +++
 .../invalid-volume-non-efspv.yaml             |  30 ++++
 .../invalid-volume-unboundpvc.yaml            |  23 +++
 .../invalid-volumemount.yaml                  |  24 +++
 .../valid-fargate-scheduler.yaml              |  69 ++++++++
 .../valid-security-context.yaml               |  32 ++++
 .../valid-volumes.yaml                        |  74 +++++++++
 17 files changed, 674 insertions(+), 13 deletions(-)
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_test.go
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-daemon-sets.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-default-scheduler.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-host-network.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-pod-securitycontext.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-ports.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-spec-fields.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-non-efspv.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-unboundpvc.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volumemount.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-fargate-scheduler.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-volumes.yaml

diff --git a/pkg/kubelet/kubelet.go b/pkg/kubelet/kubelet.go
index d02ecd13fe5..99c33064cdc 100644
--- a/pkg/kubelet/kubelet.go
+++ b/pkg/kubelet/kubelet.go
@@ -812,7 +812,7 @@ func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,
 		klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewAppArmorAdmitHandler(klet.appArmorValidator))
 	}
 	if utilfeature.DefaultFeatureGate.Enabled(features.PodSecurityValidator) {
-		klet.admitHandlers.AddPodAdmitHandler(lifecycle.NewFargatePodAdmitHandler())
+		klet.admitHandlers.AddPodAdmitHandler(lifecycle.NewFargatePodAdmitHandler(klet.kubeClient))
 	}
 	klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewNoNewPrivsAdmitHandler(klet.containerRuntime))
 	klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewProcMountAdmitHandler(klet.containerRuntime))
diff --git a/pkg/kubelet/lifecycle/fargate_pod_admit_handler.go b/pkg/kubelet/lifecycle/fargate_pod_admit_handler.go
index b058b342349..dda6d97448d 100644
--- a/pkg/kubelet/lifecycle/fargate_pod_admit_handler.go
+++ b/pkg/kubelet/lifecycle/fargate_pod_admit_handler.go
@@ -2,6 +2,7 @@ package lifecycle
 
 import (
 	corev1 "k8s.io/api/core/v1"
+	clientset "k8s.io/client-go/kubernetes"
 )
 
 var (
@@ -9,9 +10,9 @@ var (
 	unsupportedPodSpecMessage = "UnsupportedPodSpec"
 )
 
-func NewFargatePodAdmitHandler() *fargatePodAdmitHandler {
+func NewFargatePodAdmitHandler(client clientset.Interface) *fargatePodAdmitHandler {
 	return &fargatePodAdmitHandler{
-		podValidator: NewPodSpecValidator(),
+		podValidator: NewPodSpecValidator(client),
 	}
 }
 
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator.go b/pkg/kubelet/lifecycle/fargate_pod_validator.go
index aca39d8c6f5..84549771078 100644
--- a/pkg/kubelet/lifecycle/fargate_pod_validator.go
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator.go
@@ -1,10 +1,17 @@
 package lifecycle
 
 import (
+	"context"
 	"fmt"
 	"strings"
+	"time"
 
 	corev1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/wait"
+	clientset "k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/util/retry"
 )
 
 const (
@@ -30,6 +37,15 @@ var permittedCaps = map[string]bool{
 	"SYS_CHROOT":       true,
 }
 
+// Err towards more retries here since the node_authorizer/kubelet race causes a terminal failure starting the Fargate pod
+var kubeApiGetRetries = wait.Backoff{
+	Steps:    20,
+	Duration: 10 * time.Millisecond,
+	Factor:   5.0,
+	Jitter:   0.1,
+	Cap:      2 * time.Minute,
+}
+
 type validationFuncs func(*corev1.Pod) (bool, string)
 
 // PodValidator validates pods to be launched on Fargate.
@@ -37,11 +53,13 @@ type PodValidator interface {
 	Validate(*corev1.Pod) error
 }
 
-type podSpecValidator struct{}
+type podSpecValidator struct {
+	clientset.Interface
+}
 
 // NewPodSpecValidator returns a PodValidator.
-func NewPodSpecValidator() PodValidator {
-	return &podSpecValidator{}
+func NewPodSpecValidator(client clientset.Interface) PodValidator {
+	return &podSpecValidator{client}
 }
 
 // Validate checks if the pod is eligible to run on Fargate.
@@ -53,7 +71,7 @@ func (v *podSpecValidator) Validate(pod *corev1.Pod) error {
 		validateSchedulerName,
 		validateOwnerReferences,
 		validateTopLevelFields,
-		validateVolumes,
+		v.validateVolumes,
 		validateSecurityContexts,
 		validateVolumeDevices,
 		validatePorts,
@@ -199,8 +217,9 @@ func validateTopLevelFields(pod *corev1.Pod) (bool, string) {
 	return true, ""
 }
 
-func validateVolumes(pod *corev1.Pod) (bool, string) {
-	var volumeNames []string
+func (v *podSpecValidator) validateVolumes(pod *corev1.Pod) (bool, string) {
+	var invalidVolumes []string
+	namespace := pod.Namespace
 
 	for _, volume := range pod.Spec.Volumes {
 		if volume.EmptyDir == nil &&
@@ -208,18 +227,76 @@ func validateVolumes(pod *corev1.Pod) (bool, string) {
 			volume.ConfigMap == nil &&
 			volume.Projected == nil &&
 			volume.DownwardAPI == nil &&
-			volume.NFS == nil {
-			volumeNames = append(volumeNames, volume.Name)
+			volume.NFS == nil &&
+			volume.PersistentVolumeClaim == nil {
+			message := fmt.Sprintf("%v is of an unsupported volume Type", volume.Name)
+			invalidVolumes = append(invalidVolumes, message)
+			continue
+		}
+		if volume.PersistentVolumeClaim != nil {
+			validpvc, err := v.validatePersistentVolumeClaim(volume.PersistentVolumeClaim, namespace)
+			if err != nil || !validpvc {
+				invalidVolumes = append(invalidVolumes, fmt.Sprintf("%v not supported because: %v", volume.Name, err))
+			}
 		}
 	}
 
-	if len(volumeNames) != 0 {
-		message := fmt.Sprintf("volumes not supported: %s", strings.Join(volumeNames, ","))
+	if len(invalidVolumes) != 0 {
+		message := fmt.Sprintf("%s", strings.Join(invalidVolumes, ", "))
 		return false, message
 	}
 	return true, ""
 }
 
+func (v *podSpecValidator) validatePersistentVolumeClaim(claim *corev1.PersistentVolumeClaimVolumeSource, namespace string) (bool, error) {
+	shouldRetryKubeApiGet := func(err error) bool {
+		if errors.IsInvalid(err) ||
+			errors.IsGone(err) ||
+			errors.IsNotAcceptable(err) ||
+			errors.IsNotFound(err) ||
+			errors.IsBadRequest(err) {
+			return false
+		}
+		// attempt retries on other errors, especially Unauthorized/Forbidden errors due to node_authorizer/Kubelet race
+		// https://github.com/kubernetes/kubernetes/pull/87696
+		return true
+	}
+	var pvc *corev1.PersistentVolumeClaim
+	var pv *corev1.PersistentVolume
+	err := retry.OnError(kubeApiGetRetries, shouldRetryKubeApiGet, func() (err error) {
+		name := claim.ClaimName
+		ctx := context.TODO()
+		pvc, err = v.CoreV1().PersistentVolumeClaims(namespace).Get(ctx, name, metav1.GetOptions{})
+		if err != nil {
+			return err
+		}
+
+		pv, err = v.CoreV1().PersistentVolumes().Get(ctx, pvc.Spec.VolumeName, metav1.GetOptions{})
+		if err != nil {
+			return err
+		}
+		return nil
+	})
+	if err != nil {
+		return false, err
+	}
+	return isValidPVC(pvc, pv)
+}
+
+func isValidPVC(pvc *corev1.PersistentVolumeClaim, pv *corev1.PersistentVolume) (bool, error) {
+	// only PVCs that are bound to an EFS CSI Driver PV are allowed as of now
+	if pvc.Status.Phase != corev1.ClaimBound {
+		return false, fmt.Errorf("PVC %v not bound", pvc.Name)
+	}
+	if pv.Spec.CSI == nil {
+		return false, fmt.Errorf("PVC %v bound to invalid PV %v with nil CSI spec", pvc.Name, pv.Name)
+	}
+	if pv.Spec.CSI.Driver != "efs.csi.aws.com" {
+		return false, fmt.Errorf("PVC %v bound to invalid PV %v with CSI Driver: %v", pvc.Name, pv.Name, pv.Spec.CSI.Driver)
+	}
+	return true, nil
+}
+
 func validateVolumeDevices(pod *corev1.Pod) (bool, string) {
 	var invalidContainers []string
 
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_test.go b/pkg/kubelet/lifecycle/fargate_pod_validator_test.go
new file mode 100644
index 00000000000..fecac736051
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_test.go
@@ -0,0 +1,154 @@
+package lifecycle
+
+import (
+	"fmt"
+	"io/ioutil"
+	"os"
+	"path/filepath"
+	"testing"
+
+	corev1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	clientset "k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/kubernetes/fake"
+	"sigs.k8s.io/yaml"
+)
+
+var fixtureDir = "./fargate_pod_validator_testdata"
+
+const (
+	admitPodAnnotation   = "fargate.eks.amazonaws.com/admit"
+	messagePodAnnotation = "fargate.eks.amazonaws.com/admitMessage"
+)
+
+// TestPodValidation tests the podSpecValidator.
+func TestPodValidation(t *testing.T) {
+	err := filepath.Walk(fixtureDir, func(path string, info os.FileInfo, err error) error {
+		if err != nil {
+			t.Errorf("Error while walking test fixtures: %v", err)
+			return err
+		}
+		if info.IsDir() {
+			return nil
+		}
+
+		if filepath.Ext(info.Name()) != ".yaml" {
+			return nil
+		}
+		pod, err := parseFile(path)
+		if err != nil {
+			t.Errorf("Error while parsing file %s: %v", info.Name(), err)
+			return err
+		}
+		// instantiating test client here to make sure namespace matches the pod spec in the testdata .yaml
+		stopCh := make(chan struct{})
+		defer close(stopCh)
+		client := getFakeKubeClientForVolumeTests(pod.Namespace)
+		validator := NewPodSpecValidator(client)
+		t.Run(fmt.Sprintf("Pod %s in file %s", pod.Name, path), func(t *testing.T) {
+			expectedAdmitValue, ok := pod.Annotations[admitPodAnnotation]
+			if !ok {
+				t.Errorf("Pod %s in file %s is missing annotation %s", pod.Name, path, admitPodAnnotation)
+			}
+			expectedAdmitMessage, _ := pod.Annotations[messagePodAnnotation]
+			err = validator.Validate(pod)
+			admit := (err == nil)
+			var message string
+			if err != nil {
+				message = err.Error()
+			}
+			if expectedAdmitValue != fmt.Sprintf("%t", admit) {
+				t.Errorf("Pod %s in file %s expected admit %s, got %t",
+					pod.Name, path, expectedAdmitValue, admit)
+			}
+			if !admit {
+				if expectedAdmitMessage != message {
+					t.Errorf("Pod %s in file %s expected message '%s', got '%s'",
+						pod.Name, path, expectedAdmitMessage, message)
+				}
+			}
+		})
+		return nil
+	})
+	if err != nil {
+		t.Errorf("Error while walking test fixtures: %v", err)
+	}
+}
+
+func parseFile(filename string) (*corev1.Pod, error) {
+	data, err := ioutil.ReadFile(filename)
+	if err != nil {
+		return nil, err
+	}
+	pod := &corev1.Pod{}
+	err = yaml.Unmarshal(data, pod)
+	return pod, err
+}
+
+func getFakeKubeClientForVolumeTests(namespace string) clientset.Interface {
+	efsPvName := "efs-pv"
+	nonEfsPvName := "not-an-efs-pv"
+	noCSIPVName := "no-csi-pv"
+	validPVC := &corev1.PersistentVolumeClaim{
+		ObjectMeta: metav1.ObjectMeta{Name: "efs-pvc", Namespace: namespace},
+		Spec: corev1.PersistentVolumeClaimSpec{
+			VolumeName: efsPvName,
+		},
+		Status: corev1.PersistentVolumeClaimStatus{
+			Phase: corev1.ClaimBound,
+		},
+	}
+	validPV := &corev1.PersistentVolume{
+		ObjectMeta: metav1.ObjectMeta{Name: efsPvName},
+		Spec: corev1.PersistentVolumeSpec{
+			PersistentVolumeSource: corev1.PersistentVolumeSource{CSI: &corev1.CSIPersistentVolumeSource{
+				Driver: "efs.csi.aws.com",
+			}},
+		},
+	}
+	invalidPVC := &corev1.PersistentVolumeClaim{
+		ObjectMeta: metav1.ObjectMeta{Name: "invalid-pvc", Namespace: namespace},
+		Status: corev1.PersistentVolumeClaimStatus{
+			Phase: corev1.ClaimPending,
+		},
+		Spec: corev1.PersistentVolumeClaimSpec{
+			VolumeName: nonEfsPvName,
+		},
+	}
+
+	nonEFSPVC := &corev1.PersistentVolumeClaim{
+		ObjectMeta: metav1.ObjectMeta{Name: "pvc-with-nonefs-pv", Namespace: namespace},
+		Status: corev1.PersistentVolumeClaimStatus{
+			Phase: corev1.ClaimBound,
+		},
+		Spec: corev1.PersistentVolumeClaimSpec{
+			VolumeName: nonEfsPvName,
+		},
+	}
+
+	nonEFSPV := &corev1.PersistentVolume{
+		ObjectMeta: metav1.ObjectMeta{Name: nonEfsPvName},
+		Spec: corev1.PersistentVolumeSpec{
+			PersistentVolumeSource: corev1.PersistentVolumeSource{CSI: &corev1.CSIPersistentVolumeSource{
+				Driver: "ebs.csi.aws.com",
+			}},
+		},
+	}
+
+	noCSIPVC := &corev1.PersistentVolumeClaim{
+		ObjectMeta: metav1.ObjectMeta{Name: "pvc-with-nocsi-pv", Namespace: namespace},
+		Status: corev1.PersistentVolumeClaimStatus{
+			Phase: corev1.ClaimBound,
+		},
+		Spec: corev1.PersistentVolumeClaimSpec{
+			VolumeName: noCSIPVName,
+		},
+	}
+
+	noCSIPV := &corev1.PersistentVolume{
+		ObjectMeta: metav1.ObjectMeta{Name: noCSIPVName},
+		Spec:       corev1.PersistentVolumeSpec{},
+	}
+
+	return fake.NewSimpleClientset(validPVC, validPV, invalidPVC, nonEFSPVC, nonEFSPV, noCSIPVC, noCSIPV)
+}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-daemon-sets.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-daemon-sets.yaml
new file mode 100644
index 00000000000..fde948e6d1b
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-daemon-sets.yaml
@@ -0,0 +1,31 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: DaemonSet not supported"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+  ownerReferences:
+    - apiVersion: apps/v1
+      blockOwnerDeletion: true
+      controller: true
+      kind: DaemonSet
+      name: aws-node
+      uid: efc50262-f057-11e9-8dda-0a7cbbe5199a
+spec:
+  schedulerName: fargate-scheduler
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-default-scheduler.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-default-scheduler.yaml
new file mode 100644
index 00000000000..022af6cd7c1
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-default-scheduler.yaml
@@ -0,0 +1,24 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: SchedulerName is not fargate-scheduler"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: default-scheduler
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-host-network.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-host-network.yaml
new file mode 100644
index 00000000000..3bfca78d79f
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-host-network.yaml
@@ -0,0 +1,25 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: fields not supported: HostNetwork"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  hostNetwork: true
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-pod-securitycontext.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-pod-securitycontext.yaml
new file mode 100644
index 00000000000..c48f8d9319c
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-pod-securitycontext.yaml
@@ -0,0 +1,27 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: invalid SecurityContext fields: AllowPrivilegeEscalation, Privileged"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+      securityContext:
+        allowPrivilegeEscalation: true
+        privileged: true
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-ports.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-ports.yaml
new file mode 100644
index 00000000000..c58a453d799
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-ports.yaml
@@ -0,0 +1,19 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: port contains HostIP or HostPort"
+  name: hostport-pod
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+  - image: nginx:alpine
+    name: nginx
+    ports:
+    - name: http
+      port: 80
+      hostPort: 8000
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context.yaml
new file mode 100644
index 00000000000..dab0bc6f0c9
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context.yaml
@@ -0,0 +1,27 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: invalid SecurityContext fields: Capabilities added: NET_ADMIN"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+      securityContext:
+        capabilities:
+          add: ['NET_ADMIN']
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-spec-fields.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-spec-fields.yaml
new file mode 100644
index 00000000000..6abd9923313
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-spec-fields.yaml
@@ -0,0 +1,24 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: fields not supported: HostNetwork, HostPID, HostIPC"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  hostNetwork: true
+  hostPID: true
+  hostIPC: true
+  containers:
+  - image: nginx:alpine
+    name: nginx
+    volumeMounts:
+    - mountPath: /mnt/scratch
+      name: scratch
+  volumes:
+  - name: scratch
+    emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-non-efspv.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-non-efspv.yaml
new file mode 100644
index 00000000000..faf784f0df2
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-non-efspv.yaml
@@ -0,0 +1,30 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: ebs-vol is of an unsupported volume Type, invalid-vol not supported because: PVC pvc-with-nonefs-pv bound to invalid PV not-an-efs-pv with CSI Driver: ebs.csi.aws.com, invalid-vol2 not supported because: PVC pvc-with-nocsi-pv bound to invalid PV no-csi-pv with nil CSI spec"
+  name: ebs-volume-pod
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/ebs
+          name: ebs-vol
+          readOnly: false
+  volumes:
+    - name: ebs-vol
+      awsElasticBlockStore:
+        volumeID: vol-abc123
+        fsType: ext4
+    - name: invalid-vol
+      persistentVolumeClaim:
+        claimName: pvc-with-nonefs-pv
+    - name: invalid-vol2
+      persistentVolumeClaim:
+        claimName: pvc-with-nocsi-pv
\ No newline at end of file
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-unboundpvc.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-unboundpvc.yaml
new file mode 100644
index 00000000000..b13ce34f714
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-unboundpvc.yaml
@@ -0,0 +1,23 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: invalid-vol not supported because: PVC invalid-pvc not bound"
+  name: ebs-volume-pod
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/ebs
+          name: ebs-vol
+          readOnly: false
+  volumes:
+    - name: invalid-vol
+      persistentVolumeClaim:
+        claimName: invalid-pvc
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volumemount.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volumemount.yaml
new file mode 100644
index 00000000000..01ec12f3b62
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volumemount.yaml
@@ -0,0 +1,24 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: data is of an unsupported volume Type, volumeDevices not supported"
+  name: volumedevice-pod
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+  - image: alpine
+    name: alpine
+    volumeDevices:
+    - name: data
+      devicePath: /dev/xvda
+  volumes:
+    - name: data
+      awsElasticBlockStore:
+        volumeID: vol-abc123
+        fsType: ext4
+
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-fargate-scheduler.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-fargate-scheduler.yaml
new file mode 100644
index 00000000000..a60bf0e9219
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-fargate-scheduler.yaml
@@ -0,0 +1,69 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    kubernetes.io/psp: eks.privileged
+    "fargate.eks.amazonaws.com/admit": "true"
+  name: nginx-7cdd86688c-hpjs7
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+  ownerReferences:
+    - apiVersion: apps/v1
+      blockOwnerDeletion: true
+      controller: true
+      kind: ReplicaSet
+      name: nginx-7cdd86688c
+      uid: 3b6db7e0-f4f7-11e9-94f0-06226711ecc6
+spec:
+  affinity:
+    nodeAffinity:
+      requiredDuringSchedulingIgnoredDuringExecution:
+        nodeSelectorTerms:
+          - matchExpressions:
+              - key: eks.amazonaws.com/compute-type
+                operator: In
+                values:
+                  - fargate
+  containers:
+    - image: nginx:alpine
+      imagePullPolicy: IfNotPresent
+      name: nginx
+      ports:
+        - containerPort: 80
+          protocol: TCP
+      resources: {}
+      terminationMessagePath: /dev/termination-log
+      terminationMessagePolicy: File
+      volumeMounts:
+        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
+          name: default-token-dn9pj
+          readOnly: true
+  dnsPolicy: ClusterFirst
+  enableServiceLinks: true
+  nodeName: 28be3035-c95f-4a4e-8919-a920ade935f9
+  priority: 0
+  restartPolicy: Always
+  schedulerName: fargate-scheduler
+  securityContext: {}
+  serviceAccount: default
+  serviceAccountName: default
+  terminationGracePeriodSeconds: 30
+  tolerations:
+    - effect: NoSchedule
+      key: eks.amazonaws.com/compute-type
+      operator: Equal
+      value: fargate
+    - effect: NoExecute
+      key: node.kubernetes.io/not-ready
+      operator: Exists
+      tolerationSeconds: 300
+    - effect: NoExecute
+      key: node.kubernetes.io/unreachable
+      operator: Exists
+      tolerationSeconds: 300
+  volumes:
+    - name: default-token-dn9pj
+      secret:
+        defaultMode: 420
+        secretName: default-token-dn9pj
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context.yaml
new file mode 100644
index 00000000000..7f966bdffc4
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context.yaml
@@ -0,0 +1,32 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "true"
+  name: valid-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      securityContext:
+        capabilities:
+          # All ambient capabilities
+          add:
+          - "AUDIT_WRITE"
+          - "CHOWN"
+          - "DAC_OVERRIDE"
+          - "FOWNER"
+          - "FSETID"
+          - "KILL"
+          - "MKNOD"
+          - "NET_BIND_SERVICE"
+          - "NET_RAW"
+          - "SETFCAP"
+          - "SETGID"
+          - "SETPCAP"
+          - "SETUID"
+          - "SYS_CHROOT"
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-volumes.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-volumes.yaml
new file mode 100644
index 00000000000..335e08b08bb
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-volumes.yaml
@@ -0,0 +1,74 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "true"
+  name: valid-volumes
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+  - image: nginx:alpine
+    name: nginx
+    volumeMounts:
+    - mountPath: /mnt/scratch
+      name: scratch-vol
+    - mountPath: /mnt/secret
+      name: secret-vol
+    - mountPath: /mnt/config
+      name: config-vol
+    - mountPath: /mnt/projected
+      name: projected-vol
+    - mountPath: /mnt/downward
+      name: downward-vol
+    - mountPath: /mnt/nfs
+      name: nfs-vol
+  volumes:
+  - name: scratch-vol
+    emptyDir: {}
+  - name: secret-vol
+    secret:
+      secretName: my-secret
+  - name: config-vol
+    configMap:
+      name: my-config-map
+  - name: projected-vol
+    projected:
+      sources:
+      - secret:
+          name: my-secret
+          items:
+            - key: username
+              path: my-group/my-username
+      - downwardAPI:
+          items:
+            - path: "labels"
+              fieldRef:
+                fieldPath: metadata.labels
+            - path: "cpu_limit"
+              resourceFieldRef:
+                containerName: container-test
+                resource: limits.cpu
+      - configMap:
+          name: myconfigmap
+          items:
+            - key: config
+              path: my-group/my-config
+  - name: downward-vol
+    downwardAPI:
+      items:
+        - path: "labels"
+          fieldRef:
+            fieldPath: metadata.labels
+        - path: "annotations"
+          fieldRef:
+            fieldPath: metadata.annotations
+  - name: nfs-vol
+    nfs:
+      server: "fs-12345678.efs.us-west-2.amazonaws.com"
+      path: "/"
+  - name: efs-vol
+    persistentVolumeClaim:
+      claimName: efs-pvc
