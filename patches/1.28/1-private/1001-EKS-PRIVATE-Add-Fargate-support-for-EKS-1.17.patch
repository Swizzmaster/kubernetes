From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Rasita Pai <prasita@amazon.com>
Date: Wed, 13 Oct 2021 12:52:13 -0700
Subject: [PATCH] --EKS-PRIVATE-- Add Fargate support for EKS 1.17

// owner: @saranbalaji90
// alpha: TBD
//
// Enables kubelet to validate and restrict pods based on security contexts.
PodSecurityValidator featuregate.Feature = "PodSecurityValidator"
ref. https://code.amazon.com/packages/EKSDataPlaneKubernetes/logs/heads/release-1.16.8-eks

Sri Saran Balaji Vellore Rajakumar
Adding fargate support to aws cloud provider https://code.amazon.com/packages/EKSDataPlaneKubernetes/commits/cceb46f3b6a2228fabd0e3ca79bdc3b5caf43f76#

Sri Saran Balaji Vellore Rajakumar
Adding PodCPULimit check for fargate pods
https://code.amazon.com/packages/EKSDataPlaneKubernetes/commits/cb1e0f769e04b6d5ec389ff0bf92bdded5360cdb#

Sri Saran Balaji Vellore Rajakumar
Removing fargate node prefix from NodeName before invoking DescribeENI
https://code.amazon.com/packages/EKSDataPlaneKubernetes/commits/074acc3b1e409983f035abf024b3c876f5a567df#

Sri Saran Balaji Vellore Rajakumar
Support PrivateIP address in node name
https://code.amazon.com/packages/EKSDataPlaneKubernetes/commits/b0e0580a0225c5afa4a36ebec4e1ff8b631a09c7#

efs volume validation in Kubelet - cr https://code.amazon.com/reviews/CR-30564575

Signed-off-by: Jyoti Mahapatra<jyotima@amazon.com>
---
 pkg/features/kube_features.go                 |   8 +
 pkg/kubelet/apis/config/fuzzer/fuzzer.go      |   1 +
 pkg/kubelet/cm/helpers_linux.go               |  10 +-
 pkg/kubelet/kubelet.go                        |   4 +
 .../kuberuntime/fake_kuberuntime_manager.go   |   2 +
 .../kuberuntime_container_linux.go            |   3 +
 .../kuberuntime/kuberuntime_manager.go        |  13 +
 .../kuberuntime/kuberuntime_manager_test.go   |  57 +++
 .../lifecycle/fargate_pod_admit_handler.go    |  46 ++
 .../lifecycle/fargate_pod_validator.go        | 403 ++++++++++++++++++
 .../lifecycle/fargate_pod_validator_test.go   | 167 ++++++++
 .../invalid-daemon-sets.yaml                  |  31 ++
 .../invalid-default-scheduler.yaml            |  24 ++
 .../invalid-host-network.yaml                 |  25 ++
 .../invalid-pod-securitycontext.yaml          |  27 ++
 .../invalid-ports.yaml                        |  21 +
 ...d-security-context-blocked-linux-caps.yaml |  41 ++
 .../invalid-security-context-ctr.yaml         |  31 ++
 .../invalid-security-context.yaml             |  32 ++
 .../invalid-spec-fields.yaml                  |  24 ++
 .../invalid-volume-non-efspv.yaml             |  30 ++
 .../invalid-volume-unboundpvc.yaml            |  23 +
 .../invalid-volumemount.yaml                  |  26 ++
 .../valid-fargate-scheduler.yaml              |  69 +++
 ...-security-context-advanced-linux-caps.yaml |  39 ++
 .../valid-security-context.yaml               |  32 ++
 .../valid-volumes.yaml                        |  74 ++++
 pkg/kubelet/server/server.go                  |   3 +-
 pkg/kubelet/util/util.go                      |  20 +-
 29 files changed, 1279 insertions(+), 7 deletions(-)
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_admit_handler.go
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator.go
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_test.go
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-daemon-sets.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-default-scheduler.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-host-network.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-pod-securitycontext.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-ports.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context-blocked-linux-caps.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context-ctr.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-spec-fields.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-non-efspv.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-unboundpvc.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volumemount.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-fargate-scheduler.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context-advanced-linux-caps.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context.yaml
 create mode 100644 pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-volumes.yaml

diff --git a/pkg/features/kube_features.go b/pkg/features/kube_features.go
index b7c9a0570cd..bfec97cd110 100644
--- a/pkg/features/kube_features.go
+++ b/pkg/features/kube_features.go
@@ -618,6 +618,12 @@ const (
 	// Enable users to specify when a Pod is ready for scheduling.
 	PodSchedulingReadiness featuregate.Feature = "PodSchedulingReadiness"
 
+	// owner: @saranbalaji90
+	// alpha: TBD
+	//
+	// Enables kubelet to validate and restrict pods based on security contexts.
+	PodSecurityValidator featuregate.Feature = "PodSecurityValidator"
+
 	// owner: @ehashman
 	// alpha: v1.21
 	// beta: v1.22
@@ -1010,6 +1016,8 @@ var defaultKubernetesFeatureGates = map[featuregate.Feature]featuregate.FeatureS
 
 	PodSchedulingReadiness: {Default: true, PreRelease: featuregate.Beta},
 
+	PodSecurityValidator: {Default: false, PreRelease: featuregate.Alpha},
+
 	ProbeTerminationGracePeriod: {Default: true, PreRelease: featuregate.Beta}, // Default to true in beta 1.25
 
 	ProcMountType: {Default: false, PreRelease: featuregate.Alpha},
diff --git a/pkg/kubelet/apis/config/fuzzer/fuzzer.go b/pkg/kubelet/apis/config/fuzzer/fuzzer.go
index 79748b4b93f..685b7719d95 100644
--- a/pkg/kubelet/apis/config/fuzzer/fuzzer.go
+++ b/pkg/kubelet/apis/config/fuzzer/fuzzer.go
@@ -113,6 +113,7 @@ func Funcs(codecs runtimeserializer.CodecFactory) []interface{} {
 			if obj.Logging.Format == "" {
 				obj.Logging.Format = "text"
 			}
+			obj.EnableProfilingHandler = true
 			obj.EnableSystemLogHandler = true
 			obj.MemoryThrottlingFactor = utilpointer.Float64(rand.Float64())
 			obj.LocalStorageCapacityIsolation = true
diff --git a/pkg/kubelet/cm/helpers_linux.go b/pkg/kubelet/cm/helpers_linux.go
index 18b0df17bfc..cb81154d46e 100644
--- a/pkg/kubelet/cm/helpers_linux.go
+++ b/pkg/kubelet/cm/helpers_linux.go
@@ -33,7 +33,8 @@ import (
 	v1helper "k8s.io/kubernetes/pkg/apis/core/v1/helper"
 	v1qos "k8s.io/kubernetes/pkg/apis/core/v1/helper/qos"
 	kubefeatures "k8s.io/kubernetes/pkg/features"
-	"k8s.io/kubernetes/pkg/kubelet/cm/util"
+	cmutil "k8s.io/kubernetes/pkg/kubelet/cm/util"
+	util "k8s.io/kubernetes/pkg/kubelet/util"
 )
 
 const (
@@ -158,6 +159,7 @@ func ResourceConfigForPod(pod *v1.Pod, enforceCPULimits bool, cpuPeriod uint64,
 	// convert to CFS values
 	cpuShares := MilliCPUToShares(cpuRequests)
 	cpuQuota := MilliCPUToQuota(cpuLimits, int64(cpuPeriod))
+	cpuQuota = util.GetPodMaxCpuQuota(cpuQuota)
 
 	// quota is not capped when cfs quota is disabled
 	if !enforceCPULimits {
@@ -244,10 +246,10 @@ func getCgroupSubsystemsV2() (*CgroupSubsystems, error) {
 	mounts := []libcontainercgroups.Mount{}
 	mountPoints := make(map[string]string, len(controllers))
 	for _, controller := range controllers {
-		mountPoints[controller] = util.CgroupRoot
+		mountPoints[controller] = cmutil.CgroupRoot
 		m := libcontainercgroups.Mount{
-			Mountpoint: util.CgroupRoot,
-			Root:       util.CgroupRoot,
+			Mountpoint: cmutil.CgroupRoot,
+			Root:       cmutil.CgroupRoot,
 			Subsystems: []string{controller},
 		}
 		mounts = append(mounts, m)
diff --git a/pkg/kubelet/kubelet.go b/pkg/kubelet/kubelet.go
index 709e35015fb..caffbf801ab 100644
--- a/pkg/kubelet/kubelet.go
+++ b/pkg/kubelet/kubelet.go
@@ -678,6 +678,7 @@ func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,
 		*kubeCfg.MemoryThrottlingFactor,
 		kubeDeps.PodStartupLatencyTracker,
 		kubeDeps.TracerProvider,
+		lifecycle.NewPodSpecValidator(klet.kubeClient),
 	)
 	if err != nil {
 		return nil, err
@@ -872,6 +873,9 @@ func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,
 		klet.appArmorValidator = apparmor.NewValidator()
 		klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewAppArmorAdmitHandler(klet.appArmorValidator))
 	}
+	if utilfeature.DefaultFeatureGate.Enabled(features.PodSecurityValidator) {
+		klet.admitHandlers.AddPodAdmitHandler(lifecycle.NewFargatePodAdmitHandler(klet.kubeClient))
+	}
 
 	leaseDuration := time.Duration(kubeCfg.NodeLeaseDurationSeconds) * time.Second
 	renewInterval := time.Duration(float64(leaseDuration) * nodeLeaseRenewIntervalFraction)
diff --git a/pkg/kubelet/kuberuntime/fake_kuberuntime_manager.go b/pkg/kubelet/kuberuntime/fake_kuberuntime_manager.go
index 2f6ef8e6977..c794fbe082d 100644
--- a/pkg/kubelet/kuberuntime/fake_kuberuntime_manager.go
+++ b/pkg/kubelet/kuberuntime/fake_kuberuntime_manager.go
@@ -27,6 +27,7 @@ import (
 	"k8s.io/apimachinery/pkg/api/resource"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/types"
+	"k8s.io/client-go/kubernetes/fake"
 	"k8s.io/client-go/tools/record"
 	"k8s.io/client-go/util/flowcontrol"
 	"k8s.io/component-base/logs/logreduction"
@@ -115,6 +116,7 @@ func newFakeKubeRuntimeManager(runtimeService internalapi.RuntimeService, imageS
 		logReduction:           logreduction.NewLogReduction(identicalErrorDelay),
 		logManager:             logManager,
 		memoryThrottlingFactor: 0.9,
+		fargatePodValidator:    lifecycle.NewPodSpecValidator(fake.NewSimpleClientset()),
 	}
 
 	typedVersion, err := runtimeService.Version(ctx, kubeRuntimeAPIVersion)
diff --git a/pkg/kubelet/kuberuntime/kuberuntime_container_linux.go b/pkg/kubelet/kuberuntime/kuberuntime_container_linux.go
index 4c753b466f3..28bc2b34841 100644
--- a/pkg/kubelet/kuberuntime/kuberuntime_container_linux.go
+++ b/pkg/kubelet/kuberuntime/kuberuntime_container_linux.go
@@ -37,6 +37,7 @@ import (
 	kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
 	"k8s.io/kubernetes/pkg/kubelet/qos"
 	kubelettypes "k8s.io/kubernetes/pkg/kubelet/types"
+	"k8s.io/kubernetes/pkg/kubelet/util"
 )
 
 var defaultPageSize = int64(os.Getpagesize())
@@ -211,6 +212,8 @@ func (m *kubeGenericRuntimeManager) calculateLinuxResources(cpuRequest, cpuLimit
 			cpuPeriod = int64(m.cpuCFSQuotaPeriod.Duration / time.Microsecond)
 		}
 		cpuQuota := milliCPUToQuota(cpuLimit.MilliValue(), cpuPeriod)
+		cpuQuota = util.GetPodMaxCpuQuota(cpuQuota)
+
 		resources.CpuQuota = cpuQuota
 		resources.CpuPeriod = cpuPeriod
 	}
diff --git a/pkg/kubelet/kuberuntime/kuberuntime_manager.go b/pkg/kubelet/kuberuntime/kuberuntime_manager.go
index 16c08f8d2ec..612901d876b 100644
--- a/pkg/kubelet/kuberuntime/kuberuntime_manager.go
+++ b/pkg/kubelet/kuberuntime/kuberuntime_manager.go
@@ -168,6 +168,9 @@ type kubeGenericRuntimeManager struct {
 
 	// Memory throttling factor for MemoryQoS
 	memoryThrottlingFactor float64
+
+	// fargate pod validation handler
+	fargatePodValidator lifecycle.PodValidator
 }
 
 // KubeGenericRuntime is a interface contains interfaces for container runtime and command.
@@ -209,6 +212,7 @@ func NewKubeGenericRuntimeManager(
 	memoryThrottlingFactor float64,
 	podPullingTimeRecorder images.ImagePodPullingTimeRecorder,
 	tracerProvider trace.TracerProvider,
+	fargatePodValidator lifecycle.PodValidator,
 ) (KubeGenericRuntime, error) {
 	ctx := context.Background()
 	runtimeService = newInstrumentedRuntimeService(runtimeService)
@@ -236,6 +240,7 @@ func NewKubeGenericRuntimeManager(
 		memorySwapBehavior:     memorySwapBehavior,
 		getNodeAllocatable:     getNodeAllocatable,
 		memoryThrottlingFactor: memoryThrottlingFactor,
+		fargatePodValidator:    fargatePodValidator,
 	}
 
 	typedVersion, err := kubeRuntimeManager.getTypedVersion(ctx)
@@ -1182,6 +1187,14 @@ func (m *kubeGenericRuntimeManager) SyncPod(ctx context.Context, pod *v1.Pod, po
 		startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, spec.container.Name)
 		result.AddSyncResult(startContainerResult)
 
+		if utilfeature.DefaultFeatureGate.Enabled(features.PodSecurityValidator) {
+			if admit, err := m.fargatePodValidator.ValidateContainer(spec.container); !admit {
+				startContainerResult.Fail(err, err.Error())
+				klog.V(4).InfoS(fmt.Sprintf("Failed to start container due to %s in pod", err.Error()), "containerType", typeName, "container", spec.container, "pod", klog.KObj(pod))
+				return err
+			}
+		}
+
 		isInBackOff, msg, err := m.doBackOff(pod, spec.container, podStatus, backOff)
 		if isInBackOff {
 			startContainerResult.Fail(err, msg)
diff --git a/pkg/kubelet/kuberuntime/kuberuntime_manager_test.go b/pkg/kubelet/kuberuntime/kuberuntime_manager_test.go
index 120d4d14174..dcd188b7055 100644
--- a/pkg/kubelet/kuberuntime/kuberuntime_manager_test.go
+++ b/pkg/kubelet/kuberuntime/kuberuntime_manager_test.go
@@ -709,6 +709,63 @@ func TestSyncPodWithConvertedPodSysctls(t *testing.T) {
 	}
 }
 
+func TestSyncPodWhenContainerValidationFails(t *testing.T) {
+	ctx := context.Background()
+	_, _, m, err := createTestRuntimeManager()
+	assert.NoError(t, err)
+
+	volumeDevices := v1.VolumeDevice{
+		Name:       "volumeDevice",
+		DevicePath: "test/path",
+	}
+	containers := []v1.Container{
+		{
+			Name:  "container1",
+			Image: "busybox",
+			SecurityContext: &v1.SecurityContext{
+				Capabilities: &v1.Capabilities{
+					Add: []v1.Capability{"NET_ADMIN"},
+				},
+			},
+		},
+		{
+			Name:          "container2",
+			Image:         "busybox",
+			VolumeDevices: []v1.VolumeDevice{volumeDevices},
+		},
+	}
+
+	pod := &v1.Pod{
+		ObjectMeta: metav1.ObjectMeta{
+			UID:       "12345678",
+			Name:      "foo",
+			Namespace: "new",
+		},
+		Spec: v1.PodSpec{
+			Containers: containers,
+		},
+	}
+
+	podStatus, err := m.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
+	backOff := flowcontrol.NewBackOff(time.Second, time.Minute)
+	result := m.SyncPod(context.Background(), pod, podStatus, []v1.Secret{}, backOff)
+	expectErrMsgCtr1 := "invalid SecurityContext fields: Capabilities added: NET_ADMIN"
+	expectErrMsgCtr2 := "volumeDevices not supported"
+
+	assert.ErrorContainsf(t, result.Error(), "failed to \"StartContainer\" for \"container1\"", expectErrMsgCtr1)
+	assert.ErrorContainsf(t, result.Error(), "failed to \"StartContainer\" for \"container2\"", expectErrMsgCtr2)
+
+	// return error when unallowlisted advanced linux caps are added to containers
+	admit, err := m.fargatePodValidator.ValidateContainer(&containers[0])
+	assert.False(t, admit)
+	assert.EqualError(t, err, expectErrMsgCtr1)
+
+	// return error when unallowlisted advanced linux caps are added to containers
+	admit, err = m.fargatePodValidator.ValidateContainer(&containers[1])
+	assert.False(t, admit)
+	assert.EqualError(t, err, expectErrMsgCtr2)
+}
+
 func TestPruneInitContainers(t *testing.T) {
 	ctx := context.Background()
 	fakeRuntime, _, m, err := createTestRuntimeManager()
diff --git a/pkg/kubelet/lifecycle/fargate_pod_admit_handler.go b/pkg/kubelet/lifecycle/fargate_pod_admit_handler.go
new file mode 100644
index 00000000000..dda6d97448d
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_admit_handler.go
@@ -0,0 +1,46 @@
+package lifecycle
+
+import (
+	corev1 "k8s.io/api/core/v1"
+	clientset "k8s.io/client-go/kubernetes"
+)
+
+var (
+	// unsupportedPodSpec is set on reason field for pods which cannot execute on this kubelet node
+	unsupportedPodSpecMessage = "UnsupportedPodSpec"
+)
+
+func NewFargatePodAdmitHandler(client clientset.Interface) *fargatePodAdmitHandler {
+	return &fargatePodAdmitHandler{
+		podValidator: NewPodSpecValidator(client),
+	}
+}
+
+// fargatePodAdmitHandler verifies security aspects of pod spec before admitting the pod.
+type fargatePodAdmitHandler struct {
+	podValidator PodValidator
+}
+
+// Admit checks security aspects of pod spec and decides whether pod spec is safe to run on this kubelet.
+// Currently fargatePodAdmitHandler runs fixed set of validation to verify if pod can run on fargate kubelet.
+func (f *fargatePodAdmitHandler) Admit(attrs *PodAdmitAttributes) PodAdmitResult {
+
+	admit, message := f.validate(attrs.Pod)
+
+	response := PodAdmitResult{
+		Admit: admit,
+	}
+	if !admit {
+		response.Message = message
+		response.Reason = unsupportedPodSpecMessage
+	}
+	return response
+}
+
+func (f *fargatePodAdmitHandler) validate(pod *corev1.Pod) (bool, string) {
+	err := f.podValidator.Validate(pod)
+	if err != nil {
+		return false, err.Error()
+	}
+	return true, ""
+}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator.go b/pkg/kubelet/lifecycle/fargate_pod_validator.go
new file mode 100644
index 00000000000..09bbe62d028
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator.go
@@ -0,0 +1,403 @@
+package lifecycle
+
+import (
+	"context"
+	"fmt"
+	"os"
+	"strings"
+	"time"
+
+	corev1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/wait"
+	clientset "k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/util/retry"
+)
+
+const (
+	fargateSchedulerName = "fargate-scheduler"
+)
+
+// Linux capabilities permitted in container security contexts.
+// Copied from https://github.com/containerd/containerd/blob/257a7498d00827fbca08078f664cc6b4be27d7aa/oci/spec.go#L93
+var permittedCaps = map[string]bool{
+	"AUDIT_WRITE":      true,
+	"CHOWN":            true,
+	"DAC_OVERRIDE":     true,
+	"FOWNER":           true,
+	"FSETID":           true,
+	"KILL":             true,
+	"MKNOD":            true,
+	"NET_BIND_SERVICE": true,
+	"NET_RAW":          true,
+	"SETFCAP":          true,
+	"SETGID":           true,
+	"SETPCAP":          true,
+	"SETUID":           true,
+	"SYS_CHROOT":       true,
+}
+
+var advancedPermittedCaps = map[string]bool{
+	"IPC_LOCK":        true,
+	"LINUX_IMMUTABLE": true,
+	"SYS_PTRACE":      true,
+	"SYS_RESOURCE":    true,
+}
+
+// Err towards more retries here since the node_authorizer/kubelet race causes a terminal failure starting the Fargate pod
+var kubeApiGetRetries = wait.Backoff{
+	Steps:    20,
+	Duration: 10 * time.Millisecond,
+	Factor:   5.0,
+	Jitter:   0.1,
+	Cap:      2 * time.Minute,
+}
+
+type validationFuncs func(*corev1.Pod) (bool, string)
+
+// PodValidator validates pods to be launched on Fargate.
+type PodValidator interface {
+	Validate(*corev1.Pod) error
+	ValidateContainer(container *corev1.Container) (bool, error)
+}
+
+type podSpecValidator struct {
+	clientset.Interface
+}
+
+// NewPodSpecValidator returns a PodValidator.
+func NewPodSpecValidator(client clientset.Interface) PodValidator {
+	return &podSpecValidator{client}
+}
+
+// Validate checks if the pod is eligible to run on Fargate.
+func (v *podSpecValidator) Validate(pod *corev1.Pod) error {
+	var messages []string
+
+	// Run through all validators to communicate all violations.
+	validators := []validationFuncs{
+		validateSchedulerName,
+		validateOwnerReferences,
+		validateTopLevelFields,
+		v.validateVolumes,
+		validateSecurityContexts,
+		validateVolumeDevices,
+		validatePorts,
+	}
+
+	for _, fn := range validators {
+		admit, message := fn(pod)
+		if !admit {
+			messages = append(messages, message)
+		}
+	}
+
+	// All validators must pass for the pod to be admitted.
+	var err error
+	if len(messages) != 0 {
+		err = fmt.Errorf("Pod not supported: %s", strings.Join(messages, ", "))
+	}
+
+	return err
+}
+
+func validateSchedulerName(pod *corev1.Pod) (bool, string) {
+	// Scheduler name must be Fargate.
+	if pod.Spec.SchedulerName != fargateSchedulerName {
+		return false, fmt.Sprintf("SchedulerName is not %s", fargateSchedulerName)
+	}
+	return true, ""
+}
+
+func validateOwnerReferences(pod *corev1.Pod) (bool, string) {
+	ownerReferences := pod.ObjectMeta.OwnerReferences
+	if len(ownerReferences) > 0 {
+		for _, reference := range ownerReferences {
+			if reference.Kind == "DaemonSet" {
+				return false, "DaemonSet not supported"
+			}
+		}
+	}
+	return true, ""
+}
+
+func validateSecurityContexts(pod *corev1.Pod) (bool, string) {
+	var invalidFields []string
+
+	for _, container := range pod.Spec.InitContainers {
+		admitted, message := validateContainerSecurityContext(container.SecurityContext)
+		if !admitted {
+			invalidFields = append(invalidFields, message)
+		}
+	}
+	for _, container := range pod.Spec.Containers {
+		admitted, message := validateContainerSecurityContext(container.SecurityContext)
+		if !admitted {
+			invalidFields = append(invalidFields, message)
+		}
+	}
+
+	admitted, message := validatePodSecurityContext(pod.Spec.SecurityContext)
+	if !admitted {
+		invalidFields = append(invalidFields, message)
+	}
+
+	if len(invalidFields) != 0 {
+		message := fmt.Sprintf("invalid SecurityContext fields: %s", strings.Join(invalidFields, ","))
+		return false, message
+	}
+	return true, ""
+}
+
+// placeholder method. All PodSecurityContext fields are safe for warmpool fargate
+// at launch, but we may need to add restrictions based on what is added here later
+func validatePodSecurityContext(sc *corev1.PodSecurityContext) (bool, string) {
+	return true, ""
+}
+
+// Allow ambient capabilities to be added. This is useful if only one or more are desired
+// while the rest are dropped. Ex:
+//
+//	securityContext:
+//	  allowPrivilegeEscalation: false
+//	  capabilities:
+//	    add:
+//	    - NET_BIND_SERVICE
+//	    drop:
+//	    - all
+func validateAddedCapabilities(requested []corev1.Capability) (permitted bool, rejectedCaps []string) {
+	currentPermittedCaps := getPermittedCapsWithAdvancedCaps()
+	fmt.Println("Currently permitted linux capabilities:", currentPermittedCaps)
+
+	for _, req := range requested {
+		if accept, _ := currentPermittedCaps[string(req)]; !accept {
+			rejectedCaps = append(rejectedCaps, string(req))
+		}
+	}
+	if len(rejectedCaps) != 0 {
+		return false, rejectedCaps
+	}
+	return true, nil
+}
+
+func getPermittedCapsWithAdvancedCaps() map[string]bool {
+	allowedCaps := map[string]bool{}
+	for key, value := range permittedCaps {
+		allowedCaps[key] = value
+	}
+	if os.Getenv("ADVANCED_LINUX_CAPS") == "true" {
+		for key, value := range advancedPermittedCaps {
+			allowedCaps[key] = value
+		}
+	}
+	if os.Getenv("BLOCKED_LINUX_CAPS") != "" {
+		blockedCaps := strings.Split(os.Getenv("BLOCKED_LINUX_CAPS"), ",")
+		for _, blockedCap := range blockedCaps {
+			allowedCaps[blockedCap] = false
+		}
+	}
+	return allowedCaps
+}
+
+func validateContainerSecurityContext(sc *corev1.SecurityContext) (bool, string) {
+	var invalidFields []string
+
+	if sc == nil {
+		return true, ""
+	}
+
+	if sc.Capabilities != nil && len(sc.Capabilities.Add) != 0 {
+		admit, rejectedCaps := validateAddedCapabilities(sc.Capabilities.Add)
+		if !admit {
+			invalidFields = append(invalidFields, fmt.Sprintf("Capabilities added: %s", strings.Join(rejectedCaps, ", ")))
+		}
+	}
+
+	if sc.AllowPrivilegeEscalation != nil && *sc.AllowPrivilegeEscalation == true {
+		invalidFields = append(invalidFields, "AllowPrivilegeEscalation")
+	}
+
+	if sc.Privileged != nil && *sc.Privileged == true {
+		invalidFields = append(invalidFields, "Privileged")
+	}
+
+	if len(invalidFields) != 0 {
+		return false, strings.Join(invalidFields, ", ")
+	}
+	return true, ""
+}
+
+func validateTopLevelFields(pod *corev1.Pod) (bool, string) {
+	var invalidFields []string
+
+	if pod.Spec.HostNetwork == true {
+		invalidFields = append(invalidFields, "HostNetwork")
+	}
+	if pod.Spec.HostPID == true {
+		invalidFields = append(invalidFields, "HostPID")
+	}
+	if pod.Spec.HostIPC == true {
+		invalidFields = append(invalidFields, "HostIPC")
+	}
+
+	if len(invalidFields) != 0 {
+		message := fmt.Sprintf("fields not supported: %s", strings.Join(invalidFields, ", "))
+		return false, message
+	}
+	return true, ""
+}
+
+func (v *podSpecValidator) validateVolumes(pod *corev1.Pod) (bool, string) {
+	var invalidVolumes []string
+	namespace := pod.Namespace
+
+	for _, volume := range pod.Spec.Volumes {
+		if volume.EmptyDir == nil &&
+			volume.Secret == nil &&
+			volume.ConfigMap == nil &&
+			volume.Projected == nil &&
+			volume.DownwardAPI == nil &&
+			volume.NFS == nil &&
+			volume.PersistentVolumeClaim == nil {
+			message := fmt.Sprintf("%v is of an unsupported volume Type", volume.Name)
+			invalidVolumes = append(invalidVolumes, message)
+			continue
+		}
+		if volume.PersistentVolumeClaim != nil {
+			validpvc, err := v.validatePersistentVolumeClaim(volume.PersistentVolumeClaim, namespace)
+			if err != nil || !validpvc {
+				invalidVolumes = append(invalidVolumes, fmt.Sprintf("%v not supported because: %v", volume.Name, err))
+			}
+		}
+	}
+
+	if len(invalidVolumes) != 0 {
+		message := fmt.Sprintf("%s", strings.Join(invalidVolumes, ", "))
+		return false, message
+	}
+	return true, ""
+}
+
+func (v *podSpecValidator) validatePersistentVolumeClaim(claim *corev1.PersistentVolumeClaimVolumeSource, namespace string) (bool, error) {
+	shouldRetryKubeApiGet := func(err error) bool {
+		if errors.IsInvalid(err) ||
+			errors.IsGone(err) ||
+			errors.IsNotAcceptable(err) ||
+			errors.IsNotFound(err) ||
+			errors.IsBadRequest(err) {
+			return false
+		}
+		// attempt retries on other errors, especially Unauthorized/Forbidden errors due to node_authorizer/Kubelet race
+		// https://github.com/kubernetes/kubernetes/pull/87696
+		return true
+	}
+	var pvc *corev1.PersistentVolumeClaim
+	var pv *corev1.PersistentVolume
+	err := retry.OnError(kubeApiGetRetries, shouldRetryKubeApiGet, func() (err error) {
+		name := claim.ClaimName
+		ctx := context.TODO()
+		pvc, err = v.CoreV1().PersistentVolumeClaims(namespace).Get(ctx, name, metav1.GetOptions{})
+		if err != nil {
+			return err
+		}
+
+		pv, err = v.CoreV1().PersistentVolumes().Get(ctx, pvc.Spec.VolumeName, metav1.GetOptions{})
+		if err != nil {
+			return err
+		}
+		return nil
+	})
+	if err != nil {
+		return false, err
+	}
+	return isValidPVC(pvc, pv)
+}
+
+func isValidPVC(pvc *corev1.PersistentVolumeClaim, pv *corev1.PersistentVolume) (bool, error) {
+	// only PVCs that are bound to an EFS CSI Driver PV are allowed as of now
+	if pvc.Status.Phase != corev1.ClaimBound {
+		return false, fmt.Errorf("PVC %v not bound", pvc.Name)
+	}
+	if pv.Spec.CSI == nil {
+		return false, fmt.Errorf("PVC %v bound to invalid PV %v with nil CSI spec", pvc.Name, pv.Name)
+	}
+	if pv.Spec.CSI.Driver != "efs.csi.aws.com" {
+		return false, fmt.Errorf("PVC %v bound to invalid PV %v with CSI Driver: %v", pvc.Name, pv.Name, pv.Spec.CSI.Driver)
+	}
+	return true, nil
+}
+
+func validateVolumeDevices(pod *corev1.Pod) (bool, string) {
+	var invalidContainers []string
+
+	for _, container := range pod.Spec.InitContainers {
+		if len(container.VolumeDevices) > 0 {
+			invalidContainers = append(invalidContainers, container.Name)
+		}
+	}
+	for _, container := range pod.Spec.Containers {
+		if len(container.VolumeDevices) > 0 {
+			invalidContainers = append(invalidContainers, container.Name)
+		}
+	}
+	if len(invalidContainers) > 0 {
+		return false, "volumeDevices not supported"
+	}
+	return true, ""
+}
+
+func validatePort(port corev1.ContainerPort) bool {
+	if port.HostPort > 0 {
+		return false
+	}
+	if port.HostIP != "" {
+		return false
+	}
+	return true
+}
+
+// TODO: return more specific port violation messages later.
+func validatePorts(pod *corev1.Pod) (bool, string) {
+	message := "port contains HostIP or HostPort"
+
+	for _, container := range pod.Spec.InitContainers {
+		for _, port := range container.Ports {
+			if !validatePort(port) {
+				return false, message
+			}
+		}
+	}
+	for _, container := range pod.Spec.Containers {
+		for _, port := range container.Ports {
+			if !validatePort(port) {
+				return false, message
+			}
+		}
+	}
+	return true, ""
+}
+
+func (v *podSpecValidator) ValidateContainer(container *corev1.Container) (bool, error) {
+	// validate securityContext
+	admitted, message := validateContainerSecurityContext(container.SecurityContext)
+	if !admitted {
+		return false, fmt.Errorf("invalid SecurityContext fields: %s", message)
+	}
+
+	// validate volumeDevices
+	if len(container.VolumeDevices) > 0 {
+		return false, fmt.Errorf("volumeDevices not supported")
+	}
+
+	// validate ports, currently should be no-op for ephemeral containers
+	// since fields such as ports, livenessProbe, readinessProbe are disallowed on ephemeral containers
+	// https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers
+	for _, port := range container.Ports {
+		if !validatePort(port) {
+			return false, fmt.Errorf("port contains HostIP or HostPort")
+		}
+	}
+
+	return true, nil
+}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_test.go b/pkg/kubelet/lifecycle/fargate_pod_validator_test.go
new file mode 100644
index 00000000000..b7c7471a5c3
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_test.go
@@ -0,0 +1,167 @@
+package lifecycle
+
+import (
+	"fmt"
+	"io/ioutil"
+	"os"
+	"path/filepath"
+	"strconv"
+	"testing"
+
+	"github.com/stretchr/testify/assert"
+	corev1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	clientset "k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/kubernetes/fake"
+	"sigs.k8s.io/yaml"
+)
+
+var fixtureDir = "./fargate_pod_validator_testdata"
+
+const (
+	admitPodAnnotation                    = "fargate.eks.amazonaws.com/admit"
+	messagePodAnnotation                  = "fargate.eks.amazonaws.com/admitMessage"
+	advancedLinuxCapsPodAnnotation        = "fargate.eks.amazonaws.com/advancedLinuxCaps"
+	blockedAdvancedLinuxCapsPodAnnotation = "fargate.eks.amazonaws.com/blockedAdvancedLinuxCaps"
+	runContainerValidationAnnotation      = "fargate.eks.amazonaws.com/runContainerValidation"
+	containerAdmitMessageAnnotation       = "fargate.eks.amazonaws.com/containerAdmitMessage"
+)
+
+// TestPodValidation tests the podSpecValidator.
+func TestPodValidation(t *testing.T) {
+	err := filepath.Walk(fixtureDir, func(path string, info os.FileInfo, err error) error {
+		if err != nil {
+			t.Errorf("Error while walking test fixtures: %v", err)
+			return err
+		}
+		if info.IsDir() {
+			return nil
+		}
+
+		if filepath.Ext(info.Name()) != ".yaml" {
+			return nil
+		}
+		pod, err := parseFile(path)
+		if err != nil {
+			t.Errorf("Error while parsing file %s: %v", info.Name(), err)
+			return err
+		}
+		// instantiating test client here to make sure namespace matches the pod spec in the testdata .yaml
+		stopCh := make(chan struct{})
+		defer close(stopCh)
+		client := getFakeKubeClientForVolumeTests(pod.Namespace)
+		validator := NewPodSpecValidator(client)
+		t.Run(fmt.Sprintf("Pod %s in file %s", pod.Name, path), func(t *testing.T) {
+			expectedAdmitValue, ok := pod.Annotations[admitPodAnnotation]
+			if !ok {
+				t.Errorf("Pod %s in file %s is missing annotation %s", pod.Name, path, admitPodAnnotation)
+			}
+			os.Setenv("ADVANCED_LINUX_CAPS", pod.Annotations[advancedLinuxCapsPodAnnotation])
+			os.Setenv("BLOCKED_LINUX_CAPS", pod.Annotations[blockedAdvancedLinuxCapsPodAnnotation])
+			expectedAdmitMessage, _ := pod.Annotations[messagePodAnnotation]
+			err = validator.Validate(pod)
+			admit := (err == nil)
+			var message string
+			if err != nil {
+				message = err.Error()
+			}
+			if expectedAdmitValue != fmt.Sprintf("%t", admit) {
+				t.Errorf("Pod %s in file %s expected admit %s, got %t",
+					pod.Name, path, expectedAdmitValue, admit)
+			}
+			if !admit {
+				if expectedAdmitMessage != message {
+					t.Errorf("Pod %s in file %s expected message '%s', got '%s'",
+						pod.Name, path, expectedAdmitMessage, message)
+				}
+			}
+			if _, ok := pod.Annotations[runContainerValidationAnnotation]; ok {
+				admit, err = validator.ValidateContainer(&pod.Spec.Containers[0])
+				assert.Equal(t, strconv.FormatBool(admit), pod.Annotations[admitPodAnnotation])
+				assert.EqualError(t, err, pod.Annotations[containerAdmitMessageAnnotation])
+			}
+		})
+		return nil
+	})
+	if err != nil {
+		t.Errorf("Error while walking test fixtures: %v", err)
+	}
+}
+
+func parseFile(filename string) (*corev1.Pod, error) {
+	data, err := ioutil.ReadFile(filename)
+	if err != nil {
+		return nil, err
+	}
+	pod := &corev1.Pod{}
+	err = yaml.Unmarshal(data, pod)
+	return pod, err
+}
+
+func getFakeKubeClientForVolumeTests(namespace string) clientset.Interface {
+	efsPvName := "efs-pv"
+	nonEfsPvName := "not-an-efs-pv"
+	noCSIPVName := "no-csi-pv"
+	validPVC := &corev1.PersistentVolumeClaim{
+		ObjectMeta: metav1.ObjectMeta{Name: "efs-pvc", Namespace: namespace},
+		Spec: corev1.PersistentVolumeClaimSpec{
+			VolumeName: efsPvName,
+		},
+		Status: corev1.PersistentVolumeClaimStatus{
+			Phase: corev1.ClaimBound,
+		},
+	}
+	validPV := &corev1.PersistentVolume{
+		ObjectMeta: metav1.ObjectMeta{Name: efsPvName},
+		Spec: corev1.PersistentVolumeSpec{
+			PersistentVolumeSource: corev1.PersistentVolumeSource{CSI: &corev1.CSIPersistentVolumeSource{
+				Driver: "efs.csi.aws.com",
+			}},
+		},
+	}
+	invalidPVC := &corev1.PersistentVolumeClaim{
+		ObjectMeta: metav1.ObjectMeta{Name: "invalid-pvc", Namespace: namespace},
+		Status: corev1.PersistentVolumeClaimStatus{
+			Phase: corev1.ClaimPending,
+		},
+		Spec: corev1.PersistentVolumeClaimSpec{
+			VolumeName: nonEfsPvName,
+		},
+	}
+
+	nonEFSPVC := &corev1.PersistentVolumeClaim{
+		ObjectMeta: metav1.ObjectMeta{Name: "pvc-with-nonefs-pv", Namespace: namespace},
+		Status: corev1.PersistentVolumeClaimStatus{
+			Phase: corev1.ClaimBound,
+		},
+		Spec: corev1.PersistentVolumeClaimSpec{
+			VolumeName: nonEfsPvName,
+		},
+	}
+
+	nonEFSPV := &corev1.PersistentVolume{
+		ObjectMeta: metav1.ObjectMeta{Name: nonEfsPvName},
+		Spec: corev1.PersistentVolumeSpec{
+			PersistentVolumeSource: corev1.PersistentVolumeSource{CSI: &corev1.CSIPersistentVolumeSource{
+				Driver: "ebs.csi.aws.com",
+			}},
+		},
+	}
+
+	noCSIPVC := &corev1.PersistentVolumeClaim{
+		ObjectMeta: metav1.ObjectMeta{Name: "pvc-with-nocsi-pv", Namespace: namespace},
+		Status: corev1.PersistentVolumeClaimStatus{
+			Phase: corev1.ClaimBound,
+		},
+		Spec: corev1.PersistentVolumeClaimSpec{
+			VolumeName: noCSIPVName,
+		},
+	}
+
+	noCSIPV := &corev1.PersistentVolume{
+		ObjectMeta: metav1.ObjectMeta{Name: noCSIPVName},
+		Spec:       corev1.PersistentVolumeSpec{},
+	}
+
+	return fake.NewSimpleClientset(validPVC, validPV, invalidPVC, nonEFSPVC, nonEFSPV, noCSIPVC, noCSIPV)
+}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-daemon-sets.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-daemon-sets.yaml
new file mode 100644
index 00000000000..fde948e6d1b
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-daemon-sets.yaml
@@ -0,0 +1,31 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: DaemonSet not supported"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+  ownerReferences:
+    - apiVersion: apps/v1
+      blockOwnerDeletion: true
+      controller: true
+      kind: DaemonSet
+      name: aws-node
+      uid: efc50262-f057-11e9-8dda-0a7cbbe5199a
+spec:
+  schedulerName: fargate-scheduler
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-default-scheduler.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-default-scheduler.yaml
new file mode 100644
index 00000000000..022af6cd7c1
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-default-scheduler.yaml
@@ -0,0 +1,24 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: SchedulerName is not fargate-scheduler"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: default-scheduler
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-host-network.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-host-network.yaml
new file mode 100644
index 00000000000..3bfca78d79f
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-host-network.yaml
@@ -0,0 +1,25 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: fields not supported: HostNetwork"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  hostNetwork: true
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-pod-securitycontext.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-pod-securitycontext.yaml
new file mode 100644
index 00000000000..c48f8d9319c
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-pod-securitycontext.yaml
@@ -0,0 +1,27 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: invalid SecurityContext fields: AllowPrivilegeEscalation, Privileged"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+      securityContext:
+        allowPrivilegeEscalation: true
+        privileged: true
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-ports.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-ports.yaml
new file mode 100644
index 00000000000..6b99017ee66
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-ports.yaml
@@ -0,0 +1,21 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: port contains HostIP or HostPort"
+    "fargate.eks.amazonaws.com/runContainerValidation": "true"
+    "fargate.eks.amazonaws.com/containerAdmitMessage": "port contains HostIP or HostPort"
+  name: hostport-pod
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+  - image: nginx:alpine
+    name: nginx
+    ports:
+    - name: http
+      port: 80
+      hostPort: 8000
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context-blocked-linux-caps.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context-blocked-linux-caps.yaml
new file mode 100644
index 00000000000..bc6281a11ba
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context-blocked-linux-caps.yaml
@@ -0,0 +1,41 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    # This pod template is used to test advanced linux caps are allowed when ADVANCED_LINUX_CAPS is enabled
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/advancedLinuxCaps": "true"
+    "fargate.eks.amazonaws.com/blockedAdvancedLinuxCaps": "SYS_PTRACE,SYS_RESOURCE"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: invalid SecurityContext fields: Capabilities added: SYS_PTRACE, SYS_RESOURCE"
+  name: valid-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      securityContext:
+        capabilities:
+          add:
+            # All ambient capabilities
+            - "AUDIT_WRITE"
+            - "CHOWN"
+            - "DAC_OVERRIDE"
+            - "FOWNER"
+            - "FSETID"
+            - "KILL"
+            - "MKNOD"
+            - "NET_BIND_SERVICE"
+            - "NET_RAW"
+            - "SETFCAP"
+            - "SETGID"
+            - "SETPCAP"
+            - "SETUID"
+            - "SYS_CHROOT"
+            # Advanced linux capabilities
+            - "IPC_LOCK"
+            - "LINUX_IMMUTABLE"
+            - "SYS_PTRACE"
+            - "SYS_RESOURCE"
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context-ctr.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context-ctr.yaml
new file mode 100644
index 00000000000..bb2e16276ef
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context-ctr.yaml
@@ -0,0 +1,31 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: invalid SecurityContext fields: Capabilities added: NET_ADMIN, IPC_LOCK, LINUX_IMMUTABLE, SYS_PTRACE, SYS_RESOURCE"
+    "fargate.eks.amazonaws.com/runContainerValidation": "true"
+    "fargate.eks.amazonaws.com/containerAdmitMessage": "invalid SecurityContext fields: Capabilities added: NET_ADMIN, IPC_LOCK, LINUX_IMMUTABLE, SYS_PTRACE, SYS_RESOURCE"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      securityContext:
+        capabilities:
+          add:
+            - "NET_ADMIN"
+            - "IPC_LOCK"
+            - "LINUX_IMMUTABLE"
+            - "SYS_PTRACE"
+            - "SYS_RESOURCE"
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context.yaml
new file mode 100644
index 00000000000..33f32e491c6
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-security-context.yaml
@@ -0,0 +1,32 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: invalid SecurityContext fields: Capabilities added: NET_ADMIN, IPC_LOCK, LINUX_IMMUTABLE, SYS_PTRACE, SYS_RESOURCE"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  initContainers:
+    - image: nginx:alpine
+      name: nginx
+      securityContext:
+        capabilities:
+          add:
+            - "NET_ADMIN"
+            - "IPC_LOCK"
+            - "LINUX_IMMUTABLE"
+            - "SYS_PTRACE"
+            - "SYS_RESOURCE"
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/scratch
+          name: scratch
+  volumes:
+    - name: scratch
+      emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-spec-fields.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-spec-fields.yaml
new file mode 100644
index 00000000000..6abd9923313
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-spec-fields.yaml
@@ -0,0 +1,24 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: fields not supported: HostNetwork, HostPID, HostIPC"
+  name: init-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  hostNetwork: true
+  hostPID: true
+  hostIPC: true
+  containers:
+  - image: nginx:alpine
+    name: nginx
+    volumeMounts:
+    - mountPath: /mnt/scratch
+      name: scratch
+  volumes:
+  - name: scratch
+    emptyDir: {}
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-non-efspv.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-non-efspv.yaml
new file mode 100644
index 00000000000..faf784f0df2
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-non-efspv.yaml
@@ -0,0 +1,30 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: ebs-vol is of an unsupported volume Type, invalid-vol not supported because: PVC pvc-with-nonefs-pv bound to invalid PV not-an-efs-pv with CSI Driver: ebs.csi.aws.com, invalid-vol2 not supported because: PVC pvc-with-nocsi-pv bound to invalid PV no-csi-pv with nil CSI spec"
+  name: ebs-volume-pod
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/ebs
+          name: ebs-vol
+          readOnly: false
+  volumes:
+    - name: ebs-vol
+      awsElasticBlockStore:
+        volumeID: vol-abc123
+        fsType: ext4
+    - name: invalid-vol
+      persistentVolumeClaim:
+        claimName: pvc-with-nonefs-pv
+    - name: invalid-vol2
+      persistentVolumeClaim:
+        claimName: pvc-with-nocsi-pv
\ No newline at end of file
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-unboundpvc.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-unboundpvc.yaml
new file mode 100644
index 00000000000..b13ce34f714
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volume-unboundpvc.yaml
@@ -0,0 +1,23 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: invalid-vol not supported because: PVC invalid-pvc not bound"
+  name: ebs-volume-pod
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      volumeMounts:
+        - mountPath: /mnt/ebs
+          name: ebs-vol
+          readOnly: false
+  volumes:
+    - name: invalid-vol
+      persistentVolumeClaim:
+        claimName: invalid-pvc
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volumemount.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volumemount.yaml
new file mode 100644
index 00000000000..bb1b9a68423
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/invalid-volumemount.yaml
@@ -0,0 +1,26 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "false"
+    "fargate.eks.amazonaws.com/admitMessage": "Pod not supported: data is of an unsupported volume Type, volumeDevices not supported"
+    "fargate.eks.amazonaws.com/runContainerValidation": "true"
+    "fargate.eks.amazonaws.com/containerAdmitMessage": "volumeDevices not supported"
+  name: volumedevice-pod
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+  - image: alpine
+    name: alpine
+    volumeDevices:
+    - name: data
+      devicePath: /dev/xvda
+  volumes:
+    - name: data
+      awsElasticBlockStore:
+        volumeID: vol-abc123
+        fsType: ext4
+
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-fargate-scheduler.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-fargate-scheduler.yaml
new file mode 100644
index 00000000000..a60bf0e9219
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-fargate-scheduler.yaml
@@ -0,0 +1,69 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    kubernetes.io/psp: eks.privileged
+    "fargate.eks.amazonaws.com/admit": "true"
+  name: nginx-7cdd86688c-hpjs7
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+  ownerReferences:
+    - apiVersion: apps/v1
+      blockOwnerDeletion: true
+      controller: true
+      kind: ReplicaSet
+      name: nginx-7cdd86688c
+      uid: 3b6db7e0-f4f7-11e9-94f0-06226711ecc6
+spec:
+  affinity:
+    nodeAffinity:
+      requiredDuringSchedulingIgnoredDuringExecution:
+        nodeSelectorTerms:
+          - matchExpressions:
+              - key: eks.amazonaws.com/compute-type
+                operator: In
+                values:
+                  - fargate
+  containers:
+    - image: nginx:alpine
+      imagePullPolicy: IfNotPresent
+      name: nginx
+      ports:
+        - containerPort: 80
+          protocol: TCP
+      resources: {}
+      terminationMessagePath: /dev/termination-log
+      terminationMessagePolicy: File
+      volumeMounts:
+        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
+          name: default-token-dn9pj
+          readOnly: true
+  dnsPolicy: ClusterFirst
+  enableServiceLinks: true
+  nodeName: 28be3035-c95f-4a4e-8919-a920ade935f9
+  priority: 0
+  restartPolicy: Always
+  schedulerName: fargate-scheduler
+  securityContext: {}
+  serviceAccount: default
+  serviceAccountName: default
+  terminationGracePeriodSeconds: 30
+  tolerations:
+    - effect: NoSchedule
+      key: eks.amazonaws.com/compute-type
+      operator: Equal
+      value: fargate
+    - effect: NoExecute
+      key: node.kubernetes.io/not-ready
+      operator: Exists
+      tolerationSeconds: 300
+    - effect: NoExecute
+      key: node.kubernetes.io/unreachable
+      operator: Exists
+      tolerationSeconds: 300
+  volumes:
+    - name: default-token-dn9pj
+      secret:
+        defaultMode: 420
+        secretName: default-token-dn9pj
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context-advanced-linux-caps.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context-advanced-linux-caps.yaml
new file mode 100644
index 00000000000..d6ad3ce2f78
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context-advanced-linux-caps.yaml
@@ -0,0 +1,39 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    # This pod template is used to test advanced linux caps are allowed when ADVANCED_LINUX_CAPS is enabled
+    "fargate.eks.amazonaws.com/admit": "true"
+    "fargate.eks.amazonaws.com/advancedLinuxCaps": "true"
+  name: valid-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      securityContext:
+        capabilities:
+          add:
+            # All ambient capabilities
+            - "AUDIT_WRITE"
+            - "CHOWN"
+            - "DAC_OVERRIDE"
+            - "FOWNER"
+            - "FSETID"
+            - "KILL"
+            - "MKNOD"
+            - "NET_BIND_SERVICE"
+            - "NET_RAW"
+            - "SETFCAP"
+            - "SETGID"
+            - "SETPCAP"
+            - "SETUID"
+            - "SYS_CHROOT"
+            # Advanced linux capabilities
+            - "IPC_LOCK"
+            - "LINUX_IMMUTABLE"
+            - "SYS_PTRACE"
+            - "SYS_RESOURCE"
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context.yaml
new file mode 100644
index 00000000000..7f966bdffc4
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-security-context.yaml
@@ -0,0 +1,32 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "true"
+  name: valid-security-context
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+    - image: nginx:alpine
+      name: nginx
+      securityContext:
+        capabilities:
+          # All ambient capabilities
+          add:
+          - "AUDIT_WRITE"
+          - "CHOWN"
+          - "DAC_OVERRIDE"
+          - "FOWNER"
+          - "FSETID"
+          - "KILL"
+          - "MKNOD"
+          - "NET_BIND_SERVICE"
+          - "NET_RAW"
+          - "SETFCAP"
+          - "SETGID"
+          - "SETPCAP"
+          - "SETUID"
+          - "SYS_CHROOT"
diff --git a/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-volumes.yaml b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-volumes.yaml
new file mode 100644
index 00000000000..335e08b08bb
--- /dev/null
+++ b/pkg/kubelet/lifecycle/fargate_pod_validator_testdata/valid-volumes.yaml
@@ -0,0 +1,74 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  annotations:
+    "fargate.eks.amazonaws.com/admit": "true"
+  name: valid-volumes
+  namespace: eks-fargate-namespace
+  labels:
+    "eks.amazonaws.com/fargate-profile": "my-profile"
+spec:
+  schedulerName: fargate-scheduler
+  containers:
+  - image: nginx:alpine
+    name: nginx
+    volumeMounts:
+    - mountPath: /mnt/scratch
+      name: scratch-vol
+    - mountPath: /mnt/secret
+      name: secret-vol
+    - mountPath: /mnt/config
+      name: config-vol
+    - mountPath: /mnt/projected
+      name: projected-vol
+    - mountPath: /mnt/downward
+      name: downward-vol
+    - mountPath: /mnt/nfs
+      name: nfs-vol
+  volumes:
+  - name: scratch-vol
+    emptyDir: {}
+  - name: secret-vol
+    secret:
+      secretName: my-secret
+  - name: config-vol
+    configMap:
+      name: my-config-map
+  - name: projected-vol
+    projected:
+      sources:
+      - secret:
+          name: my-secret
+          items:
+            - key: username
+              path: my-group/my-username
+      - downwardAPI:
+          items:
+            - path: "labels"
+              fieldRef:
+                fieldPath: metadata.labels
+            - path: "cpu_limit"
+              resourceFieldRef:
+                containerName: container-test
+                resource: limits.cpu
+      - configMap:
+          name: myconfigmap
+          items:
+            - key: config
+              path: my-group/my-config
+  - name: downward-vol
+    downwardAPI:
+      items:
+        - path: "labels"
+          fieldRef:
+            fieldPath: metadata.labels
+        - path: "annotations"
+          fieldRef:
+            fieldPath: metadata.annotations
+  - name: nfs-vol
+    nfs:
+      server: "fs-12345678.efs.us-west-2.amazonaws.com"
+      path: "/"
+  - name: efs-vol
+    persistentVolumeClaim:
+      claimName: efs-pvc
diff --git a/pkg/kubelet/server/server.go b/pkg/kubelet/server/server.go
index beaaeabb4a8..d113c767079 100644
--- a/pkg/kubelet/server/server.go
+++ b/pkg/kubelet/server/server.go
@@ -529,8 +529,7 @@ func (s *Server) InstallDebuggingHandlers() {
 
 	s.addMetricsBucketMatcher("containerLogs")
 	ws = new(restful.WebService)
-	ws.
-		Path("/containerLogs")
+	ws.Path("/containerLogs")
 	ws.Route(ws.GET("/{podNamespace}/{podID}/{containerName}").
 		To(s.getContainerLogs).
 		Operation("getContainerLogs"))
diff --git a/pkg/kubelet/util/util.go b/pkg/kubelet/util/util.go
index c2969a51126..1f5edaee902 100644
--- a/pkg/kubelet/util/util.go
+++ b/pkg/kubelet/util/util.go
@@ -18,8 +18,10 @@ package util
 
 import (
 	"fmt"
-
+	"k8s.io/apimachinery/pkg/api/resource"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/klog/v2"
+	"os"
 )
 
 // FromApiserverCache modifies <opts> so that the GET request will
@@ -43,3 +45,19 @@ func GetNodenameForKernel(hostname string, hostDomainName string, setHostnameAsF
 	}
 	return kernelHostname, nil
 }
+
+// GetPodMaxCpuQuota, returns the max CPU quota that can be allocated to pod.
+func GetPodMaxCpuQuota(currentCpuQuota int64) int64 {
+	fargatePodCPULimit := os.Getenv("FARGATE_POD_CPU_LIMIT")
+
+	if fargatePodCPULimit != "" {
+		fargatePodCPUResource := resource.MustParse(fargatePodCPULimit)
+		fargatePodCPUQuota := fargatePodCPUResource.MilliValue() * 100
+
+		if currentCpuQuota > fargatePodCPUQuota {
+			klog.Infof("updating cpuQuota for pod from %v to %v", currentCpuQuota, fargatePodCPUQuota)
+			return fargatePodCPUQuota
+		}
+	}
+	return currentCpuQuota
+}
